from dataclasses import dataclass
from typing import Optional
import pandas as pd
from statistics import mean, stdev, median

import properscoring as ps
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, brier_score_loss, average_precision_score, roc_auc_score
# WARNING: mean_squared_error and mean_squared_log_error are deprecated.
from views_forecasts.extensions import *


@dataclass
class EvaluationMetrics:
    """
    A data class for storing and managing evaluation metrics for time series forecasting models.
    
    Attributes:
        MSE (Optional[float]): Mean Squared Error.
        MAE (Optional[float]): Mean Absolute Error.
        MSLE (Optional[float]): Mean Squared Logarithmic Error.
        KLD (Optional[float]): Kullback-Leibler Divergence.
        Jeffreys (Optional[float]): Jeffreys Divergence.
        CRPS (Optional[float]): Continuous Ranked Probability Score.
        Brier (Optional[float]): Brier Score.
        AP (Optional[float]): Average Precision.
        AUC (Optional[float]): Area Under the ROC Curve.
        ensemble_weight_reg (Optional[float]): Weight for regression ensemble models.
        ensemble_weight_class (Optional[float]): Weight for classification ensemble models.
    """

    MSE: Optional[float] = None
    MAE: Optional[float] = None
    MSLE: Optional[float] = None
    KLD: Optional[float] = None
    Jeffreys: Optional[float] = None
    CRPS: Optional[float] = None
    Brier: Optional[float] = None
    AP: Optional[float] = None
    AUC: Optional[float] = None
    ensemble_weight_reg: Optional[float] = None
    ensemble_weight_class: Optional[float] = None

    @classmethod
    def make_evaluation_dict(cls, steps=36) -> dict:
        """
        Generates a dictionary of EvaluationMetrics instances for a specified number of forecasting steps.

        This method facilitates the batch creation of metric containers for multiple forecasting steps, initializing them with None.

        Args:
            steps (int): The number of forecasting steps for which to generate evaluation metrics. Defaults to 36.

        Returns:
            dict: A dictionary where each key is a step label (e.g., 'step01', 'step02', ...) and each value is an instance of EvaluationMetrics.

        Example:
            >>> from utils_evaluation_metrics import EvaluationMetrics
            >>> evaluation_dict = EvaluationMetrics.make_evaluation_dict(steps=36)
            >>> evaluation_dict['step01'].MSE = sklearn.metrics.mean_squared_error(step01_y_true, step01_y_pred)
            >>> evaluation_dict['step02'].MSE = sklearn.metrics.mean_squared_error(step02_y_true, step02_y_pred)
            >>> ...
            
        """
        return {f"step{str(i).zfill(2)}": cls() for i in range(1, steps + 1)}

    @staticmethod
    def evaluation_dict_to_dataframe(evaluation_dict: dict) -> pd.DataFrame:
        """
        Converts a dictionary of EvaluationMetrics instances into a pandas DataFrame for easy analysis.

        This static method transforms a structured dictionary of evaluation metrics into a DataFrame, where each row corresponds to a forecasting step and columns represent different metrics.

        Args:
            evaluation_dict (dict): A dictionary of EvaluationMetrics instances, typically generated by the make_evaluation_dict class method.

        Returns:
            pd.DataFrame: A pandas DataFrame where each row indexes a forecasting step and columns correspond to the various metrics stored in EvaluationMetrics.

        Example:
            >>> evaluation_df = EvaluationMetrics.evaluation_dict_to_dataframe(evaluation_dict)

        """
        return pd.DataFrame.from_dict(evaluation_dict, orient='index')

# TBD: Align with metrics discussed in workshop

    @staticmethod
    def calculate_aggregate_metrics(evaluation_dict: dict) -> dict:
        metrics_aggregate = {
            'mean': {},
            'std': {},
            'median': {}
        }

        for metric in EvaluationMetrics.__annotations__.keys():
            metric_values = [getattr(evaluation, metric) for evaluation in evaluation_dict.values() if getattr(evaluation, metric) is not None]
            if metric_values: 
                metrics_aggregate['mean'][metric] = mean(metric_values)
                metrics_aggregate['std'][metric] = stdev(metric_values)
                metrics_aggregate['median'][metric] = median(metric_values)
            else:
                metrics_aggregate['mean'][metric] = None
                metrics_aggregate['std'][metric] = None
                metrics_aggregate['median'][metric] = None

        return metrics_aggregate

    @staticmethod
    def output_metrics(evaluation_dict):
        aggregate = EvaluationMetrics.calculate_aggregate_metrics(evaluation_dict)
        step_metrics_dict = {step: vars(metrics) for step, metrics in evaluation_dict.items()}
        step_metrics_dict['mean'] = aggregate['mean']
        step_metrics_dict['std'] = aggregate['std']
        step_metrics_dict['median'] = aggregate['median']
        return step_metrics_dict


def generate_metric_dict(df, config):
    """
    Generates a dictionary of evaluation metrics for a given forecasting configuration and dataset.

    Args:
        df (pd.DataFrame): A pandas DataFrame containing the forecasted values and ground truth.
        config (dict): A dictionary containing the forecasting configuration parameters.

    Returns:
        evaluation_dict (dict): A dictionary of EvaluationMetrics instances for each forecasting step.
        df_evaluation_dict (pd.DataFrame): A pandas DataFrame containing the evaluation metrics for each forecasting step.

    Note:
        ! This function is temporary for the stepshifter model.
        ! Change the metrics to those discussed previously.
        For logged targets, calculating MSE is actually MSLE.
        KLD and Jeffreys divergence are measures used to quantify the difference between two probability distributions. Why do we calculate these metrics in the context of forecasting?
        Brier score is used for binary and categorical outcomes that can be structured as true or false
        There are no classes in data, so we cannot calculate roc_auc_score, ap_score
    """

    evaluation_dict = EvaluationMetrics.make_evaluation_dict(steps=config["steps"][-1])
    for step in config["steps"]:
        evaluation_dict[f"step{str(step).zfill(2)}"].MSE = mean_squared_error(df[config["depvar"]], df[f"step_pred_{step}"])
        evaluation_dict[f"step{str(step).zfill(2)}"].MAE = mean_absolute_error(df[config["depvar"]], df[f"step_pred_{step}"])
        # evaluation_dict[f"step{str(step).zfill(2)}"].MSLE = mean_squared_log_error(df[config["depvar"]], df[f"step_pred_{step}"])
        evaluation_dict[f"step{str(step).zfill(2)}"].CRPS = ps.crps_ensemble(df[config["depvar"]], df[f"step_pred_{step}"]).mean()
        # evaluation_dict[f"step{str(step).zfill(2)}"].Brier = brier_score_loss(df[config["depvar"]], df[f"step_pred_{step}"])
        # evaluation_dict[f"step{str(step).zfill(2)}"].AUC = roc_auc_score(df[config["depvar"]], df[f"step_pred_{step}"])
        # evaluation_dict[f"step{str(step).zfill(2)}"].AP = average_precision_score(df[config["depvar"]], df[f"step_pred_{step}"])
    evaluation_dict = EvaluationMetrics.output_metrics(evaluation_dict)
    df_evaluation_dict = EvaluationMetrics.evaluation_dict_to_dataframe(evaluation_dict)
    return evaluation_dict, df_evaluation_dict