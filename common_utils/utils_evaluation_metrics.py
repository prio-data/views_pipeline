from dataclasses import dataclass
from typing import Optional
import pandas as pd
from statistics import mean, stdev, median



# MUST BE ALIGNED WITH THE METRICS WE DECIDE TO USE IN THE WORKSHOP!!!!

@dataclass
class EvaluationMetrics:
    """
    A data class for storing and managing evaluation metrics for time series forecasting models.
    
    Attributes:
        MSE (Optional[float]): Mean Squared Error.
        MAE (Optional[float]): Mean Absolute Error.
        MSLE (Optional[float]): Mean Squared Logarithmic Error.
        KLD (Optional[float]): Kullback-Leibler Divergence.
        Jeffreys (Optional[float]): Jeffreys Divergence.
        CRPS (Optional[float]): Continuous Ranked Probability Score.
        Brier (Optional[float]): Brier Score.
        AP (Optional[float]): Average Precision.
        AUC (Optional[float]): Area Under the ROC Curve.
        ensemble_weight_reg (Optional[float]): Weight for regression ensemble models.
        ensemble_weight_class (Optional[float]): Weight for classification ensemble models.
    """

    MSE: Optional[float] = None
    MAE: Optional[float] = None
    MSLE: Optional[float] = None
    KLD: Optional[float] = None
    Jeffreys: Optional[float] = None
    CRPS: Optional[float] = None
    Brier: Optional[float] = None
    AP: Optional[float] = None
    AUC: Optional[float] = None
    ensemble_weight_reg: Optional[float] = None
    ensemble_weight_class: Optional[float] = None

    @classmethod
    def make_evaluation_dict(cls, steps=36) -> dict:
        """
        Generates a dictionary of EvaluationMetrics instances for a specified number of forecasting steps.

        This method facilitates the batch creation of metric containers for multiple forecasting steps, initializing them with None.

        Args:
            steps (int): The number of forecasting steps for which to generate evaluation metrics. Defaults to 36.

        Returns:
            dict: A dictionary where each key is a step label (e.g., 'step01', 'step02', ...) and each value is an instance of EvaluationMetrics.

        Example:
            >>> from utils_evaluation_metrics import EvaluationMetrics
            >>> evaluation_dict = EvaluationMetrics.make_evaluation_dict(steps=36)
            >>> evaluation_dict['step01'].MSE = sklearn.metrics.mean_squared_error(step01_y_true, step01_y_pred)
            >>> evaluation_dict['step02'].MSE = sklearn.metrics.mean_squared_error(step02_y_true, step02_y_pred)
            >>> ...
            
        """
        return {f"step{str(i).zfill(2)}": cls() for i in range(1, steps + 1)}

    @staticmethod
    def evaluation_dict_to_dataframe(evaluation_dict: dict) -> pd.DataFrame:
        """
        Converts a dictionary of EvaluationMetrics instances into a pandas DataFrame for easy analysis.

        This static method transforms a structured dictionary of evaluation metrics into a DataFrame, where each row corresponds to a forecasting step and columns represent different metrics.

        Args:
            evaluation_dict (dict): A dictionary of EvaluationMetrics instances, typically generated by the make_evaluation_dict class method.

        Returns:
            pd.DataFrame: A pandas DataFrame where each row indexes a forecasting step and columns correspond to the various metrics stored in EvaluationMetrics.

        Example:
            >>> evaluation_df = EvaluationMetrics.evaluation_dict_to_dataframe(evaluation_dict)

        """
        return pd.DataFrame.from_dict(evaluation_dict, orient='index')

    @staticmethod
    def calculate_aggregate_metrics(evaluation_dict: dict) -> dict:
        metrics_aggregate = {
            'mean': {},
            'std': {},
            'median': {}
        }

        for metric in EvaluationMetrics.__annotations__.keys():
            metric_values = [getattr(evaluation, metric) for evaluation in evaluation_dict.values() if getattr(evaluation, metric) is not None]
            if metric_values: 
                metrics_aggregate['mean'][metric] = mean(metric_values)
                metrics_aggregate['std'][metric] = stdev(metric_values)
                metrics_aggregate['median'][metric] = median(metric_values)
            else:
                metrics_aggregate['mean'][metric] = None
                metrics_aggregate['std'][metric] = None
                metrics_aggregate['median'][metric] = None

        return metrics_aggregate

    @staticmethod
    def output_metrics(evaluation_dict):
        aggregate = EvaluationMetrics.calculate_aggregate_metrics(evaluation_dict)
        step_metrics_dict = {step: vars(metrics) for step, metrics in evaluation_dict.items()}
        step_metrics_dict['mean'] = aggregate['mean']
        step_metrics_dict['std'] = aggregate['std']
        step_metrics_dict['median'] = aggregate['median']
        return step_metrics_dict