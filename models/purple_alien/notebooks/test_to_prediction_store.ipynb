{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpd_2023 environment\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch    \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# now get AUROC and average precision for the probas and mse for the ln's\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, brier_score_loss\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# set path to the utils\n",
    "PATH = Path(\"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/notebooks/test_to_prediction_store.ipynb\")\n",
    "sys.path.insert(0, str(Path(*[i for i in PATH.parts[:PATH.parts.index(\"views_pipeline\")+1]]) / \"common_utils\")) # PATH_COMMON_UTILS  \n",
    "from set_path import setup_project_paths, setup_data_paths\n",
    "setup_project_paths(PATH)\n",
    "\n",
    "# now import the local functions\n",
    "from utils_df_to_vol_conversion import df_to_vol, df_vol_conversion_test, plot_vol\n",
    "from utils_wandb import generate_wandb_log_dict \n",
    "from utils_evaluation_metrics import EvaluationMetrics\n",
    "from utils_model_outputs import ModelOutputs\n",
    "from utils_hydranet_outputs import output_to_df, plot_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so this works... lest recreat the monthly metrics for all features and the plot from above\n",
    "#make this more general at some point... \n",
    "\n",
    "#def plot_metrics(df_all, feature = 0):\n",
    "#\n",
    "#    \"\"\"\n",
    "#    Plots MSE, Average Precision, ROC AUC, and Brier Score for each month from log_dict_list.\n",
    "#\n",
    "#    Args:\n",
    "#        log_dict_list (list of dict): List of dictionaries with monthly metrics.\n",
    "#        num_months (int): Number of months to plot.\n",
    "#    \"\"\"\n",
    "#\n",
    "#    # Initialize lists to store metrics for each month\n",
    "#    mse_list = []\n",
    "#    ap_list = []\n",
    "#    auc_list = []\n",
    "#    brier_list = []\n",
    "#\n",
    "#    df_all[\"month\"] = df_all[\"step\"] #super quick fix, super lazy\n",
    "#\n",
    "#\n",
    "#    # Iterate over the log_dict_list and extract the metrics\n",
    "#    for i in df_all[\"month\"].unique():\n",
    "#\n",
    "#        y_score = df_all[df_all[\"month\"] == i][f\"y_score_{feature}\"]\n",
    "#        y_score_prob = df_all[df_all[\"month\"] == i][f\"y_score_prob_{feature}\"]\n",
    "#        y_true = df_all[df_all[\"month\"] == i][f\"y_true_{feature}\"]\n",
    "#        y_true_binary = df_all[df_all[\"month\"] == i][f\"y_true_binary_{feature}\"]\n",
    "#\n",
    "#        mse = mean_squared_error(y_true, y_score)\n",
    "#        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "#        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "#        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "#\n",
    "#\n",
    "#        mse_list.append(mse)\n",
    "#        ap_list.append(ap)\n",
    "#        auc_list.append(auc)\n",
    "#        brier_list.append(brier)\n",
    "#\n",
    "#    # Create subplots\n",
    "#    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "#\n",
    "#    # Plot MSE\n",
    "#    axs[0, 0].plot(range(1, len(mse_list) + 1), mse_list, marker='o', color='b', label='MSE')\n",
    "#    axs[0, 0].set_title('Mean Squared Error')\n",
    "#    axs[0, 0].set_xlabel('Month')\n",
    "#    axs[0, 0].set_ylabel('MSE')\n",
    "#    axs[0, 0].legend()\n",
    "#    axs[0, 0].grid(True)\n",
    "#\n",
    "#    # Plot Average Precision\n",
    "#    axs[0, 1].plot(range(1, len(ap_list) + 1), ap_list, marker='o', color='g', label='Average Precision')\n",
    "#    axs[0, 1].set_title('Average Precision Score')\n",
    "#    axs[0, 1].set_xlabel('Month')\n",
    "#    axs[0, 1].set_ylabel('AP Score')\n",
    "#    axs[0, 1].legend()\n",
    "#    axs[0, 1].grid(True)\n",
    "#\n",
    "#    # Plot ROC AUC\n",
    "#    axs[1, 0].plot(range(1, len(auc_list) + 1), auc_list, marker='o', color='r', label='ROC AUC')\n",
    "#    axs[1, 0].set_title('ROC AUC Score')\n",
    "#    axs[1, 0].set_xlabel('Month')\n",
    "#    axs[1, 0].set_ylabel('AUC Score')\n",
    "#    axs[1, 0].legend()\n",
    "#    axs[1, 0].grid(True)\n",
    "#\n",
    "#    # Plot Brier Score\n",
    "#    axs[1, 1].plot(range(1, len(brier_list) + 1), brier_list, marker='o', color='m', label='Brier Score')\n",
    "#    axs[1, 1].set_title('Brier Score Loss')\n",
    "#    axs[1, 1].set_xlabel('Month')\n",
    "#    axs[1, 1].set_ylabel('Brier Score')\n",
    "#    axs[1, 1].legend()\n",
    "#    axs[1, 1].grid(True)\n",
    "#\n",
    "#    # add a title\n",
    "#    plt.suptitle(f'Metrics for Feature {feature} Over {df_all[\"month\"].max()} Months', fontsize=16)\n",
    "#\n",
    "#    # Adjust layout\n",
    "#    plt.tight_layout()\n",
    "#\n",
    "#    # Show plots\n",
    "#    plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions \n",
    "We start with all well functioning well documented functions - note where the \"live\" and if anything needs to be adjusted for the if/when the function goes into a script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should move stuff to common_utils now if it is done... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that you volumn can be loaded and is correct.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_df = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_df.pkl\"\n",
    "\n",
    "# get the df from the pickle file in raw_data\n",
    "df = pd.read_pickle(PATH_df)\n",
    "\n",
    "# turn the df1 into a volume\n",
    "vol = df_to_vol(df)\n",
    "\n",
    "df_vol_conversion_test(df, vol)\n",
    "\n",
    "plot_vol(vol, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values for the month_id in ]\n",
    "np.unique(vol[:,:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then figure out the thing below with doing stuff without the ourput tensor... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "# Stuff THAT WORKS!!!\n",
    "\n",
    "Now it is basically about:\n",
    "- take the bit loop below, and make sure it works for both eval and forecasing using the meta thing\n",
    "- Also there should be a simple way to turn as out_put_dict into a metric dict... \n",
    "    - Can you think future output drift detection into this or is that completely seperate?\n",
    "- Then package everything nice and neat\n",
    "- Distribute to relevant/appropiate scripts\n",
    "- Make sure to ubdate the eval rutine to corrospond\n",
    "- Then implement the forecasting rutine... Finally. \n",
    "\n",
    "\n",
    "## old notes\n",
    "\n",
    "## start reading here!!! :\n",
    "\n",
    "This is the thing that need to be the fundament.\n",
    "First I need to see that this works with the storage_array or another way of retaining or retreaving pg_id and month_id\n",
    "One possible way to do this is by using the meta_tensor you have started on below\n",
    "An alternative would be to to make a full full_tensor. I.e one that contains both the 3 prime feature and the \"meta\" fatures\n",
    "Ones we have proven we can get month_id and pgm back we need to see a out-of-sampel solution where we go beyond what is in the df\n",
    "And then I think we need to go back streamline the evaluation process so it follows. WHile doing so I must\n",
    "    \n",
    "- keep true forecasting firmly in the mind\n",
    "- Think about what, if anything, can be abstracted out to common_utils for the sake of getting it right for the stepshifters - maybe wandb stuff\n",
    "- And while ad it, make sure you use you new data class to store the monthly metrics -------------------------------------------------This now--------------------------\n",
    "- The more you can absract out and make general, the simple it should be\n",
    "- And while you add it, make sure to check if the partitions are the same as in paper. If they are, get the results down and finish paper.\n",
    "\n",
    "\n",
    "# WHAT YOU ARE REALLY DOING:\n",
    "- Aligning evaluation with forecasting to make a common appraoch\n",
    "- MAke sure that logging and uploading (wandb) of metrics is consice and consitant\n",
    "- Make sure that logging and uplaoding (predicion store and other?) os out outs from all partitions - whether true forecast or not - is consice and consistent\n",
    "- You might want a data class thing of outputs (like the one uo have ofr metrics). Generel for all models forecsating conflict\n",
    "- You want to make everythign as generel/abstract as possible - especially WandB stuff!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_posterior_dict = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/generated/posterior_dict_36_calibration_20240613_165106.pkl\"\n",
    "\n",
    "# get the posterior_dict from the pickle file in generated\n",
    "with open(PATH_posterior_dict, 'rb') as f:\n",
    "    posterior_dict = pickle.load(f)\n",
    "\n",
    "month_range = 36 # MAGIC NUMBER ALERT - this is the number of months in the future we are forecasting\n",
    "\n",
    "# get the three lists from the posterior_dict - we make the out_of_sample_vol later\n",
    "posterior_list, posterior_list_class, _ = posterior_dict['posterior_list'], posterior_dict['posterior_list_class'], posterior_dict['out_of_sample_vol'] # obviously there will be no out_of_sample_vol with true forecasting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_tensor(views_vol): #, config, device):\n",
    "\n",
    "    \"\"\"Uses to get the features for the full tensor\n",
    "    Used for out-of-sample predictions for both evaluation and forecasting, depending on the run_type (partition). \n",
    "    The test tensor is of size 1 x config.time_steps x config.input_channels x 180 x 180.\"\"\"\n",
    "\n",
    "    ln_best_sb_idx = 5#config.first_feature_idx # 5 = ln_best_sb\n",
    "    last_feature_idx = ln_best_sb_idx + month_range #config.input_channels\n",
    "\n",
    "    print(f'views_vol shape {views_vol.shape}')\n",
    "\n",
    "    # THIS IS WHERE YOU LOOSE THE OTHE FEATURES!!!!\n",
    "    full_tensor = torch.tensor(views_vol).float().unsqueeze(dim=0).permute(0,1,4,2,3)[:, :, ln_best_sb_idx:last_feature_idx, :, :] \n",
    "\n",
    "    # Make a metadata tensor with evrything else\n",
    "    metadata_tensor = torch.tensor(views_vol).float().unsqueeze(dim=0).permute(0,1,4,2,3)[:, :, :ln_best_sb_idx, :, :]\n",
    "\n",
    "    print(f'full_tensor shape {full_tensor.shape}')\n",
    "\n",
    "    return full_tensor, metadata_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the views_vol from the df\n",
    "views_vol = df_to_vol(df)\n",
    "views_vol = views_vol.copy() # why the fuck face this works I swear I have no idea\n",
    "\n",
    "# get the full_tensor and the new meta_data:tendor\n",
    "full_tensor, metadata_tensor = get_full_tensor(views_vol ) #, config, device) # better cal this evel tensor\n",
    "print(views_vol.shape)\n",
    "\n",
    "# Get the out of sample vol and the out of sampele meta_vol <---------------------------- THIS IS CENTRAL, BECAUSE TO REAL FORECASTING TO WORK YOU JUST NEED TO GEN A SYNTH out_of_sample_meta_vol\n",
    "out_of_sample_vol = full_tensor[:,-month_range:,:,:,:] #.cpu().numpy() # From the test tensor get the out-of-sample time_steps.\n",
    "out_of_sample_meta_vol = metadata_tensor[:,-month_range:,:,:,:]\n",
    "\n",
    "print(out_of_sample_vol.shape)\n",
    "print(out_of_sample_meta_vol.shape)\n",
    "\n",
    "# Merge full_tensor and metadata_tensor along the feature dimension - just a test\n",
    "full_t_new = np.concatenate((out_of_sample_meta_vol, out_of_sample_vol), axis=2)\n",
    "\n",
    "print(f'Shape of merged tensor: {full_t_new.shape}') # lets trust that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 36\n",
    "eval = True\n",
    "\n",
    "if eval:\n",
    "    dict_of_evel_dicts = {}\n",
    "    dict_of_evel_dicts = {k: EvaluationMetrics.make_evaluation_dict(steps=steps) for k in [\"sb\", \"ns\", \"os\"]}\n",
    "\n",
    "dict_of_outputs_dicts = {}\n",
    "dict_of_outputs_dicts = {k: ModelOutputs.make_output_dict(steps=steps) for k in [\"sb\", \"ns\", \"os\"]}\n",
    "\n",
    "# Get mean and std\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "#NEW\n",
    "log_dict_list = []\n",
    "#feature_dict_list = []\n",
    "\n",
    "for t in range(mean_array.shape[0]): #  0 of mean array is the temporal dim    \n",
    "    \n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = t +1 # 1 indexed, bc the first step is 1 month ahead\n",
    "\n",
    "    for i, j in enumerate(dict_of_evel_dicts.keys()): # this is the same as the above but with the dict keys\n",
    "\n",
    "        step = f\"step{str(t+1).zfill(2)}\"\n",
    "        \n",
    "        # get the scores\n",
    "        y_score = mean_array[t,i,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[t,i,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[t,i,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[t,i,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        # see this is the out of sample vol - fine for evaluation but not for forecasting\n",
    "        # but also the place where you get the pgm.. \n",
    "\n",
    "        if eval:\n",
    "            y_true = out_of_sample_vol[:,t,i,:,:].numpy().reshape(-1)  # nu 180x180 . dim 0 is time     THE TRICK IS NOW TO USE A df -> vol and not out_of_sample_vol...\n",
    "            y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "            # in theorty you could just use the metadata tensor to get pg and c id here\n",
    "            pg_id = out_of_sample_meta_vol[:,t,0,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 0 is pg_id\n",
    "            c_id = out_of_sample_meta_vol[:,t,4,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 4 is c_id\n",
    "            month_id = out_of_sample_meta_vol[:,t,3,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 3 is month_id\n",
    "\n",
    "            dict_of_outputs_dicts[j][step].y_true = y_true\n",
    "            dict_of_outputs_dicts[j][step].y_true_binary = y_true_binary\n",
    "\n",
    "        else: # you need to make sure this works for forecasting\n",
    "            # in theorty you could just use the metadata tensor to get pg and c id here\n",
    "            pg_id = out_of_sample_meta_vol[:,t,0,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 0 is pg_id\n",
    "            c_id = out_of_sample_meta_vol[:,t,4,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 4 is c_id\n",
    "            month_id = out_of_sample_meta_vol[:,t,3,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 3 is month_id\n",
    "\n",
    "\n",
    "        dict_of_outputs_dicts[j][step].y_score = y_score\n",
    "        dict_of_outputs_dicts[j][step].y_score_prob = y_score_prob\n",
    "        dict_of_outputs_dicts[j][step].y_var = y_var\n",
    "        dict_of_outputs_dicts[j][step].y_var_prob = y_var_prob\n",
    "\n",
    "        dict_of_outputs_dicts[j][step].pg_id = pg_id # in theory this should be in the right order\n",
    "        dict_of_outputs_dicts[j][step].c_id = c_id # in theory this should be in the right order\n",
    "        dict_of_outputs_dicts[j][step].step = t +1 # 1 indexed, bc the first step is 1 month ahead\n",
    "        dict_of_outputs_dicts[j][step].month_id = month_id\n",
    "\n",
    "        if eval:   \n",
    "\n",
    "            dict_of_evel_dicts[j][step].MSE = mean_squared_error(y_true, y_score)\n",
    "            dict_of_evel_dicts[j][step].AP = average_precision_score(y_true_binary, y_score_prob)\n",
    "            dict_of_evel_dicts[j][step].AUC = roc_auc_score(y_true_binary, y_score_prob)\n",
    "            dict_of_evel_dicts[j][step].Brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "            log_dict = generate_wandb_log_dict(log_dict, dict_of_evel_dicts, j, step)\n",
    "\n",
    "    if eval:\n",
    "        log_dict_list.append(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output_to_df(dict_of_outputs_dicts):\n",
    "#     \n",
    "#     \"\"\"\n",
    "#     Converts the dictionary of model outputs into a consolidated pandas DataFrame, formatted for HydraNet.\n",
    "# \n",
    "#     This function takes dictionaries of model outputs for different target variables ('sb', 'ns', 'os'), \n",
    "#     converts them into separate DataFrames, renames columns to distinguish between different targets, \n",
    "#     and then merges them into a single DataFrame. The merged DataFrame excludes ocean cells (where `c_id == 0`).\n",
    "# \n",
    "#     Args:\n",
    "#         dict_of_outputs_dicts (dict): A dictionary containing sub-dictionaries of model outputs \n",
    "#                                       for different targets ('sb', 'ns', 'os'). Each sub-dictionary \n",
    "#                                       should be structured with keys as steps and values as `ModelOutputs` instances.\n",
    "# \n",
    "#     Returns:\n",
    "#         df_all (pd.DataFrame): A DataFrame where columns from different targets are suffixed with '0', '1', or '2' \n",
    "#                       respectively. The DataFrame is cleaned to exclude ocean cells and has columns \n",
    "#                       properly typed for use in HydraNet.\n",
    "# \n",
    "#     Example:\n",
    "#         >>> dict_of_outputs_dicts = {\n",
    "#                 'sb': {'step01': ModelOutputs(...), ...},\n",
    "#                 'ns': {'step01': ModelOutputs(...), ...},\n",
    "#                 'os': {'step01': ModelOutputs(...), ...}\n",
    "#             }\n",
    "#         >>> df_full = output_to_df(dict_of_outputs_dicts)\n",
    "#         >>> print(df_full.head())\n",
    "#     \"\"\"\n",
    "# \n",
    "#      # Example usage with 'sb', 'ns', 'os'\n",
    "#     df_sb = ModelOutputs.output_dict_to_dataframe(dict_of_outputs_dicts[\"sb\"])\n",
    "#     df_ns = ModelOutputs.output_dict_to_dataframe(dict_of_outputs_dicts[\"ns\"])\n",
    "#     df_os = ModelOutputs.output_dict_to_dataframe(dict_of_outputs_dicts[\"os\"])\n",
    "# \n",
    "#     # SO FROM HERE IT GETS VERY HydraNet SPECIFIC. \n",
    "#     common_cols = [\"pg_id\", \"c_id\", \"month_id\", \"step\"]\n",
    "# \n",
    "#     # rename the columns so that the onse in df_test2 ends with a 0 and the ones in df_test3 ends with a 1. don't change the common columns\n",
    "#     df_sb.columns = [f\"{i}_sb\" if i not in common_cols else i for i in df_sb.columns]\n",
    "#     df_ns.columns = [f\"{i}_ns\" if i not in common_cols else i for i in df_ns.columns]\n",
    "#     df_os.columns = [f\"{i}_os\" if i not in common_cols else i for i in df_os.columns]\n",
    "# \n",
    "#     # drop the pg_id and c_id columns from df_ns and df_os - bc concat is faster than merge when they are sorted the same way.\n",
    "#     df_sb = df_sb.drop(columns=common_cols)\n",
    "#     df_ns = df_ns.drop(columns=common_cols)\n",
    "# \n",
    "#     # merge the dataframes\n",
    "#     df_all = pd.concat([df_sb, df_ns, df_os], axis=1)\n",
    "# \n",
    "#     # drop ocean cells, i.e. where c_id == 0\n",
    "#     df_all = df_all[df_all[\"c_id\"] != 0]\n",
    "# \n",
    "#     # no you can just drop it\n",
    "#     df_all = df_all.reset_index(drop=True)\n",
    "# \n",
    "#     # change all columns to float\n",
    "#     df_all = df_all.astype(float)\n",
    "# \n",
    "#     # make the binary columns integers\n",
    "#     df_all = df_all.astype({\"y_true_binary_sb\": int, \"y_true_binary_ns\": int, \"y_true_binary_os\": int, \"month_id\" : int, \"step\" : int})\n",
    "# \n",
    "#     # print the df\n",
    "#     df_all\n",
    "# \n",
    "#     return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = output_to_df(dict_of_outputs_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_precision_score(df_full[\"y_true_binary_sb\"], df_full[\"y_score_prob_sb\"]))\n",
    "print(average_precision_score(df_full[\"y_true_binary_ns\"], df_full[\"y_score_prob_ns\"]))\n",
    "print(average_precision_score(df_full[\"y_true_binary_os\"], df_full[\"y_score_prob_os\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "plot_metrics(df_full, \"sb\")\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_full, \"ns\")\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_full, \"os\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD but GOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test2 = pd.DataFrame.from_dict(feature_dict_list_2).apply(pd.Series.explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = pd.DataFrame.from_dict(feature_dict_list).apply(pd.Series.explode)\n",
    "\n",
    "#df_test2 = df_test4\n",
    "\n",
    "# reset the index and keep the old index as a column month\n",
    "df_test2 = df_test2.reset_index().rename(columns = {\"index\": \"month\"})\n",
    "\n",
    "df_test2 # we don't konw if the month order is correct yet... But I highly suspect that it is.\n",
    "\n",
    "# check all datatyps in columns\n",
    "df_test2.dtypes\n",
    "\n",
    "# change all columns to float\n",
    "df_test2 = df_test2.astype(float)\n",
    "\n",
    "# check all datatyps in columns\n",
    "df_test2.dtypes\n",
    "\n",
    "# make the binary columns integers\n",
    "df_test2 = df_test2.astype({\"y_true_binary0\": int, \"y_true_binary1\": int, \"y_true_binary2\": int})\n",
    "\n",
    "# check all datatyps in columns\n",
    "df_test2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_precision_score(df_test2[\"y_true_binary0\"], df_test2[\"y_score_prob0\"]))\n",
    "print(average_precision_score(df_test2[\"y_true_binary1\"], df_test2[\"y_score_prob1\"]))\n",
    "print(average_precision_score(df_test2[\"y_true_binary2\"], df_test2[\"y_score_prob2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "plot_metrics(df_test2, 0)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 1)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD BUT USEFUL CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Df to vol, vol to df\n",
    "\n",
    "Lest check that we can load the df, create a volumn, and then recreate af df again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def df_to_vol(df):\n",
    "#    \"\"\"\n",
    "#    Converts a DataFrame into a 4D volume array for spatial-temporal data representation.\n",
    "#    The volume array is [height, width, n_months, n_features].\n",
    "#\n",
    "#    Args:\n",
    "#        df (pd.DataFrame): The input DataFrame containing spatial-temporal data with columns\n",
    "#                           'abs_row', 'abs_col', 'abs_month', 'pg_id', 'col', 'row',\n",
    "#                           'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', and 'ln_os_best'.\n",
    "#\n",
    "#    Returns:\n",
    "#        np.ndarray: The volume representation of the DataFrame with shape \n",
    "#                    [n_months, height, width, n_features].\n",
    "#                    The specific shape will be [n_months, 180, 180, 8].\n",
    "#    \"\"\"\n",
    "#    month_first = df['month_id'].min()\n",
    "#    month_last = df['month_id'].max()\n",
    "#    month_range = month_last - month_first + 1\n",
    "#    space_range = 180\n",
    "#    features_num = 8  # Should match the number of features in the DataFrame\n",
    "#\n",
    "#    vol = np.zeros([space_range, space_range, month_range, features_num])\n",
    "#\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 0] = df['pg_id']\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 1] = df['col']\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 2] = df['row']\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 3] = df['month_id']\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 4] = df['c_id']\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 5] = df['ln_sb_best']\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 6] = df['ln_ns_best']\n",
    "#    vol[df['abs_row'], df['abs_col'], df['abs_month'], 7] = df['ln_os_best']\n",
    "#\n",
    "#    vol = np.flip(vol, axis=0)  # Flip the rows, so north is up.\n",
    "#    vol = np.transpose(vol, (2, 0, 1, 3))  # Move the month dimension to the front.\n",
    "#\n",
    "#    print(f'Volume of shape {vol.shape} created. Should be (n_months, 180, 180, 8)')\n",
    "#\n",
    "#    return vol\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_to_df(vol):\n",
    "    \"\"\"\n",
    "    Converts a 4D volume array back into a DataFrame for spatial-temporal data.\n",
    "    The volume array is expected to have dimensions [n_months, height, width, n_features].\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The input 4D volume array to be converted, with shape \n",
    "                          [n_months, height, width, n_features].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame representation of the volume array containing columns\n",
    "                      'pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best'.\n",
    "                      DataFrame is cleaned to remove rows where 'pg_id' is 0.\n",
    "    \"\"\"\n",
    "    n_months, height, width, n_features = vol.shape\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'pg_id': vol[:, :, :, 0].flatten(),\n",
    "        'col': vol[:, :, :, 1].flatten(),\n",
    "        'row': vol[:, :, :, 2].flatten(),\n",
    "        'month_id': vol[:, :, :, 3].flatten(),\n",
    "        'c_id': vol[:, :, :, 4].flatten(),\n",
    "        'ln_sb_best': vol[:, :, :, 5].flatten(),\n",
    "        'ln_ns_best': vol[:, :, :, 6].flatten(),\n",
    "        'ln_os_best': vol[:, :, :, 7].flatten()\n",
    "    })\n",
    "\n",
    "    # Correct the data types\n",
    "    df['pg_id'] = df['pg_id'].astype(int)\n",
    "    df['col'] = df['col'].astype(int)\n",
    "    df['row'] = df['row'].astype(int)\n",
    "    df['month_id'] = df['month_id'].astype(int)\n",
    "    df['c_id'] = df['c_id'].astype(int)\n",
    "\n",
    "    # Remove rows where 'pg_id' is 0\n",
    "    df = df[df['pg_id'] != 0]\n",
    "\n",
    "    print(f'DataFrame of shape {df.shape} created. Should be (n_months * 180 * 180, 8)')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_vol_test(df, vol):\n",
    "    \"\"\"\n",
    "    Unit test to verify the conversion between DataFrame and volume array.\n",
    "    Checks if the original DataFrame and the DataFrame created from the volume are equivalent.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The original DataFrame.\n",
    "        vol (np.ndarray): The 4D volume array created from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the result of the equivalence test.\n",
    "    \"\"\"\n",
    "    # Make a copy of the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Proof of concept: Check if the copy is the same as the original\n",
    "    print(\"Original DataFrame equals its copy:\", df.equals(df_copy))\n",
    "\n",
    "    # Convert the volume back into a DataFrame\n",
    "    df_recreated = vol_to_df(vol)\n",
    "\n",
    "    # Trim the original DataFrame to match the features of the recreated DataFrame\n",
    "    df_trimmed = df[['pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']]\n",
    "\n",
    "    # Sort both DataFrames by 'pg_id' and 'month_id'\n",
    "    df_trimmed = df_trimmed.sort_values(by=['pg_id', 'month_id'])\n",
    "    df_recreated = df_recreated.sort_values(by=['pg_id', 'month_id'])\n",
    "\n",
    "    # Reset the index to ensure alignment\n",
    "    df_trimmed = df_trimmed.reset_index(drop=True)\n",
    "    df_recreated = df_recreated.reset_index(drop=True)\n",
    "\n",
    "    # Check if the two DataFrames are the same\n",
    "    is_equal = df_trimmed.equals(df_recreated)\n",
    "    print(\"Trimmed original DataFrame equals recreated DataFrame from volume:\", is_equal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vol(vol, month_range):\n",
    "    \"\"\"\n",
    "    Plots slices of the 4D volume array for the specified month range.\n",
    "    Displays different feature maps for each time step in separate subplots.\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The input 4D volume array with shape [n_months, height, width, n_features].\n",
    "        month_range (int): The number of slices (time steps) to plot.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plots.\n",
    "    \"\"\"\n",
    "    features_titles = ['pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']\n",
    "    n_features = vol.shape[-1]\n",
    "\n",
    "    # get sub_df of the lasst month_range months\n",
    "    vol = vol[-month_range:, :, :, :]\n",
    "\n",
    "    for i in range(month_range):\n",
    "        fig, ax = plt.subplots(1, n_features, figsize=(20, 4))\n",
    "        \n",
    "        for j in range(min(n_features, vol.shape[-1])):  # Handle cases where there are fewer than 7 features\n",
    "            im = ax[j].imshow(vol[i, :, :, j], cmap='rainbow', vmin= vol[:, :, :, j].min(), vmax= vol[:, :, :, j].max())\n",
    "            ax[j].set_title(features_titles[j] if j < len(features_titles) else f'Feature {j}')\n",
    "            # plt.colorbar(im, ax=ax[j])\n",
    "\n",
    "        # Adding title with specific adjustment\n",
    "        fig.suptitle(f'Time Step {i + 1}', fontsize=16, y=1.05)  # Adjust `y` for title position\n",
    "\n",
    "        # remove ticks\n",
    "        for a in ax:\n",
    "            a.set_xticks([])\n",
    "            a.set_yticks([])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.subplots_adjust(left=0.1, right=1, top=0.85, bottom=0.55, wspace=0.2, hspace=-0)\n",
    "        plt.tight_layout(pad=2.0, rect=[0, 0, 1, 0.9])  # `rect` adjusts the position of subplots\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the raw data\n",
    "PATH = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_df.pkl\"\n",
    "\n",
    "# get the df from the pickle file in raw_data\n",
    "df = pd.read_pickle(PATH)\n",
    "\n",
    "# turn the df1 into a volume\n",
    "vol = df_to_vol(df)\n",
    "\n",
    "df_vol_test(df, vol)\n",
    "\n",
    "plot_vol(vol, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forecast storage test\n",
    "Now we test how we can make a storage array for the 4D forcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_vol(vol, month_range=36):\n",
    "    \"\"\"\n",
    "    Generates a fake prediction volume for testing purposes by extracting the last three features from the input volume.\n",
    "    Assumes the last three features represent `sb`, `ns`, and `os`.\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The input 4D volume array with shape [n_months, height, width, n_features].\n",
    "        n_months (int): The number of months to include in the fake volume. Default is 36.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A volume array with the last three features, shape [32, height, width, 3].\n",
    "                    Represents a subset of the original volume for testing.\n",
    "    \"\"\"\n",
    "    # Extract the last three features from the volume\n",
    "    fake_vol = vol[-month_range:, :, :, 5:]  \n",
    "\n",
    "    return fake_vol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_forecast_storage_vol(df, month_range = 36, true_forecast = False):\n",
    "    \"\"\"\n",
    "    Creates a forecast storage volume based on the last month of data in the DataFrame.\n",
    "    The volume is repeated for the specified `month_range` with incrementally adjusted month IDs.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing spatial-temporal data.\n",
    "                           Expected columns include 'abs_row', 'abs_col', 'abs_month', 'pg_id', 'col', \n",
    "                           'row', 'month_id', 'c_id'.\n",
    "        month_range (int): The number of months to forecast into the future. Default is 36.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The forecast storage volume with shape [month_range, 180, 180, 5].\n",
    "                    Each time slice in the volume represents a future month based on the last month of data.\n",
    "    \"\"\"\n",
    "    # Infer the last month_id from the DataFrame\n",
    "    last_month_id = df['month_id'].max()\n",
    "\n",
    "    # Create a sub DataFrame of only the last month\n",
    "    sub_df = df[df['month_id'] == last_month_id].copy()\n",
    "\n",
    "    # Initialize the volume array\n",
    "    space_range = 180\n",
    "    features_num = 5  # Adjust this based on the number of features you have\n",
    "\n",
    "    # Create the zero array with only the last month\n",
    "    vol = np.zeros([space_range, space_range, 1, features_num])\n",
    "\n",
    "    # Adjust abs_month to 0 for the initial volume\n",
    "    sub_df['adjusted_abs_month'] = 0\n",
    "\n",
    "    # Populate the volume array with the data from the DataFrame\n",
    "    vol[sub_df['abs_row'], sub_df['abs_col'], sub_df['adjusted_abs_month'], 0] = sub_df['pg_id']\n",
    "    vol[sub_df['abs_row'], sub_df['abs_col'], sub_df['adjusted_abs_month'], 1] = sub_df['col']\n",
    "    vol[sub_df['abs_row'], sub_df['abs_col'], sub_df['adjusted_abs_month'], 2] = sub_df['row']\n",
    "    vol[sub_df['abs_row'], sub_df['abs_col'], sub_df['adjusted_abs_month'], 3] = sub_df['month_id'] \n",
    "    vol[sub_df['abs_row'], sub_df['abs_col'], sub_df['adjusted_abs_month'], 4] = sub_df['c_id']\n",
    "\n",
    "    # Stack the volume to the desired month range\n",
    "    vol = np.repeat(vol, month_range, axis=2)\n",
    "\n",
    "#    if true_forecast:\n",
    "    # Adjust the month_id with an increment of 1\n",
    " #       for i in range(month_range):\n",
    "            #vol[:, :, i, 3] = last_month_id + i + 1 # to get one month after the last observed month\n",
    "\n",
    "#    else:\n",
    "#        pass\n",
    "\n",
    "\n",
    "    # THIS IS A WIERD THING AND THERE COULD BE A BUG HERE.... :\n",
    "    # Adjust the month_id with an increment of 1\n",
    "    for i in range(month_range):\n",
    "        vol[:, :, i, 3] = last_month_id + i + 1 # to get one month after the last observed month\n",
    "\n",
    "    # Reorient and transpose\n",
    "    vol = np.flip(vol, axis=0)\n",
    "    vol = np.transpose(vol, (2, 0, 1, 3))\n",
    "\n",
    "    print(f'Volume of shape {vol.shape} created. Should be ({month_range}, 180, 180, {features_num})')\n",
    "\n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vol(forecast_storage_vol, vol_fake):\n",
    "    \"\"\"\n",
    "    Merges a forecast volume with an existing forecast storage volume.\n",
    "    Combines the features from `vol_fake` with `vol` along the feature axis.\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The forecast storage volume with shape [n_months, height, width, n_features].\n",
    "        vol_fake (np.ndarray): The forecast volume to be merged with shape [n_months, height, width, n_features_fake].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The merged volume with shape [n_months, height, width, n_features + n_features_fake].\n",
    "    \"\"\"\n",
    "    # Merge the forecast volume with the storage volume along the feature axis\n",
    "    full_vol = np.concatenate([forecast_storage_vol, vol_fake], axis=-1)\n",
    "\n",
    "    # print the shape of the full volume\n",
    "    print(f'Volume of shape {full_vol.shape} created. Should be ({forecast_storage_vol.shape[0]}, 180, 180, {forecast_storage_vol.shape[3] + vol_fake.shape[3]})')\n",
    "\n",
    "    return full_vol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vol_equal(vol, full_vol):\n",
    "    \"\"\"\n",
    "    Unit test to verify the merging of two volumes.\n",
    "    Checks if the original volume and the merged volume are equivalent.\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The original volume.\n",
    "        full_vol (np.ndarray): The merged volume.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the result of the equivalence test.\n",
    "    \"\"\"\n",
    "\n",
    "    #print the shape of the volumes\n",
    "    print(vol.shape)\n",
    "    print(full_vol.shape)\n",
    "\n",
    "    # trim original volume to the same shape as the full volume - ie. the last n months\n",
    "    month_range = full_vol.shape[0]\n",
    "    vol_trimmed = vol[-month_range:, :, :, :]\n",
    "\n",
    "    # print the shape of the trimmed volume\n",
    "    print(vol_trimmed.shape)\n",
    "\n",
    "    # now go through each feature individually and check if they are the same\n",
    "\n",
    "    list_features = ['pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']\n",
    "\n",
    "    for i in range(vol_trimmed.shape[-1]):\n",
    "        print(f\"Feature {i}, {list_features[i]} equal:\", np.array_equal(vol_trimmed[:, :, :, i], full_vol[:, :, :, i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vol_comparison(vol, new_vol, month_range=36):\n",
    "    \"\"\"\n",
    "    Plots a comparison of slices from two 4D volume arrays for the specified month range.\n",
    "    Displays different feature maps for each time step in separate subplots for both volumes.\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The original 4D volume array with shape [n_months, height, width, n_features].\n",
    "        new_vol (np.ndarray): The new 4D volume array to compare with, with the same shape as `vol`.\n",
    "        month_range (int): The number of slices (time steps) to plot. Default is 36.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays the plots.\n",
    "    \"\"\"\n",
    "    features_titles = ['pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']\n",
    "    n_features = vol.shape[-1]\n",
    "\n",
    "    # Ensure the volumes cover the last month_range months\n",
    "    vol = vol[-month_range:, :, :, :]\n",
    "    new_vol = new_vol[-month_range:, :, :, :]\n",
    "\n",
    "    for i in range(month_range):\n",
    "        fig, ax = plt.subplots(2, n_features, figsize=(20, 7))  # 2 rows, n_features columns\n",
    "        \n",
    "        for j in range(n_features):  # Adjusted to use n_features directly\n",
    "            # Plot the original volume in the first row\n",
    "            im1 = ax[0, j].imshow(vol[i, :, :, j], cmap='rainbow',\n",
    "                                  vmin=vol[:, :, :, j].min(), vmax=vol[:, :, :, j].max())\n",
    "            ax[0, j].set_title(features_titles[j] if j < len(features_titles) else f'Feature {j}')\n",
    "            #plt.colorbar(im1, ax=ax[0, j])\n",
    "\n",
    "            # Plot the new volume in the second row\n",
    "            im2 = ax[1, j].imshow(new_vol[i, :, :, j], cmap='rainbow',\n",
    "                                  vmin=new_vol[:, :, :, j].min(), vmax=new_vol[:, :, :, j].max())\n",
    "            ax[1, j].set_title(f'New {features_titles[j]}' if j < len(features_titles) else f'New Feature {j}')\n",
    "            #plt.colorbar(im2, ax=ax[1, j])\n",
    "\n",
    "        # Adding title with specific adjustment\n",
    "        fig.suptitle(f'Time Step {i + 1}', fontsize=16, y=1.05)  # Adjust `y` for title position\n",
    "\n",
    "        # Remove ticks\n",
    "        for a in ax.flat:\n",
    "            a.set_xticks([])\n",
    "            a.set_yticks([])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.subplots_adjust(left=0.05, right=0.95, top=0.85, bottom=0.15, wspace=0.2, hspace=0.4)\n",
    "        plt.tight_layout(pad=2.0, rect=[0, 0, 1, 0.95])  # `rect` adjusts the position of subplots\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_range = 36\n",
    "\n",
    "vol_fake = generate_fake_vol(vol, month_range=month_range)\n",
    "\n",
    "#print shape of vol_fake\n",
    "print(vol_fake.shape)\n",
    "\n",
    "# make the forecast storage volume\n",
    "forecast_storage_vol = make_forecast_storage_vol(df, month_range=month_range)\n",
    "\n",
    "#print shape of forecast_storage_vol\n",
    "print(forecast_storage_vol.shape)\n",
    "\n",
    "# merge the forecast storage volume with the forecast volume\n",
    "new_vol = merge_vol(forecast_storage_vol, vol_fake)\n",
    "\n",
    "#print shape of full_vol\n",
    "print(new_vol.shape)\n",
    "\n",
    "# Check that the full vol is equal to the original vol (sliced correctly)\n",
    "check_vol_equal(vol, new_vol)\n",
    "\n",
    "# plot the volume slices\n",
    "# plot_vol_comparison(vol, new_vol, month_range=month_range) # works and cornfirms that the two volumes are the same (except for month_id, which is expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now construct the vol form a df, and we can reconstruct the df from the vol. We can also make a forecasting storage and merge that with (fake) predictions to get a vol similar to the original vol. Now must take that back to a df. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new_vol to df\n",
    "\n",
    "Now, create a new vol and make into a df and ceck that this df can corrospond to the original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start from scracth\n",
    "PATH = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_data.pkl\"\n",
    "df = pd.read_pickle(PATH)\n",
    "\n",
    "vol = df_to_vol(df)\n",
    "\n",
    "vol_fake = generate_fake_vol(vol, month_range=month_range)\n",
    "\n",
    "forecast_storage_vol = make_forecast_storage_vol(df, month_range=month_range)\n",
    "\n",
    "new_vol = merge_vol(forecast_storage_vol, vol_fake)\n",
    "\n",
    "check_vol_equal(vol, new_vol)\n",
    "\n",
    "df_new = vol_to_df(new_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"month_id\"].max())\n",
    "(df_new[\"month_id\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_range = 36\n",
    "month_max = df[\"month_id\"].max()\n",
    "month_array = np.arange(month_max - month_range +1, month_max+1)\n",
    "df_sub = df[df['month_id'].isin(month_array)]\n",
    "\n",
    "# only keep the features that are in the new volume\n",
    "df_sub = df_sub[['pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']]\n",
    "\n",
    "df_sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort both dataframes by pg_id and month_id\n",
    "df_sub = df_sub.sort_values(by=['pg_id', 'month_id'])\n",
    "df_new = df_new.sort_values(by=['pg_id', 'month_id'])\n",
    "\n",
    "# check which columns are not equal \n",
    "for i in range(df_sub.shape[1]):\n",
    "    print(f\"Feature {i}, {df_sub.columns[i]} equal:\", np.array_equal(df_sub.iloc[:, i], df_new.iloc[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good. Now we need to add actual predictions instead of \"fake_predictions\" which were just the last subset of observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# posterior dict to vol to df...\n",
    "Load the correct posterio_dict and the original df (you could prolly use the saved vol, but fuck it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n",
    "just some small changes to the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_to_df_new(vol):\n",
    "    \"\"\"\n",
    "    Converts a 4D volume array back into a DataFrame for spatial-temporal data.\n",
    "    The volume array is expected to have dimensions [n_months, height, width, n_features].\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The input 4D volume array to be converted, with shape \n",
    "                          [n_months, height, width, n_features].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame representation of the volume array containing columns\n",
    "                      'pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best'.\n",
    "                      DataFrame is cleaned to remove rows where 'pg_id' is 0.\n",
    "    \"\"\"\n",
    "    n_months, height, width, n_features = vol.shape\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'pg_id': vol[:, :, :, 0].flatten(),\n",
    "        'col': vol[:, :, :, 1].flatten(),\n",
    "        'row': vol[:, :, :, 2].flatten(),\n",
    "        'month_id': vol[:, :, :, 3].flatten(),\n",
    "        'c_id': vol[:, :, :, 4].flatten(),\n",
    "        'ln_sb_pred': vol[:, :, :, 5].flatten(),\n",
    "        'ln_ns_pred': vol[:, :, :, 6].flatten(),\n",
    "        'ln_os_pred': vol[:, :, :, 7].flatten(),\n",
    "        'proba_os_pred': vol[:, :, :, 8].flatten(),\n",
    "        'proba_ns_pred': vol[:, :, :, 9].flatten(),\n",
    "        'proba_sb_pred': vol[:, :, :, 10].flatten(),\n",
    "    })\n",
    "\n",
    "    # Correct the data types\n",
    "    df['pg_id'] = df['pg_id'].astype(int)\n",
    "    df['col'] = df['col'].astype(int)\n",
    "    df['row'] = df['row'].astype(int)\n",
    "    df['month_id'] = df['month_id'].astype(int)\n",
    "    df['c_id'] = df['c_id'].astype(int)\n",
    "\n",
    "    # Remove rows where 'pg_id' is 0\n",
    "    df = df[df['pg_id'] != 0]\n",
    "\n",
    "    print(f'DataFrame of shape {df.shape} created. Should be (n_months * 180 * 180, 8)')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_to_df_oos(vol):\n",
    "    \"\"\"\n",
    "    Converts a 4D volume array back into a DataFrame for spatial-temporal data.\n",
    "    The volume array is expected to have dimensions [n_months, height, width, n_features].\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The input 4D volume array to be converted, with shape \n",
    "                          [n_months, height, width, n_features].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame representation of the volume array containing columns\n",
    "                      'pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best'.\n",
    "                      DataFrame is cleaned to remove rows where 'pg_id' is 0.\n",
    "    \"\"\"\n",
    "    n_months, height, width, n_features = vol.shape\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'pg_id': vol[:, :, :, 0].flatten(),\n",
    "        'col': vol[:, :, :, 1].flatten(),\n",
    "        'row': vol[:, :, :, 2].flatten(),\n",
    "        'month_id': vol[:, :, :, 3].flatten(),\n",
    "        'c_id': vol[:, :, :, 4].flatten(),\n",
    "        'ln_sb_best_oos': vol[:, :, :, 5].flatten(),\n",
    "        'ln_ns_best_oos': vol[:, :, :, 6].flatten(),\n",
    "        'ln_os_best_oos': vol[:, :, :, 7].flatten(),\n",
    "    })\n",
    "\n",
    "    # Correct the data types\n",
    "    df['pg_id'] = df['pg_id'].astype(int)\n",
    "    df['col'] = df['col'].astype(int)\n",
    "    df['row'] = df['row'].astype(int)\n",
    "    df['month_id'] = df['month_id'].astype(int)\n",
    "    df['c_id'] = df['c_id'].astype(int)\n",
    "\n",
    "    # Remove rows where 'pg_id' is 0\n",
    "    df = df[df['pg_id'] != 0]\n",
    "\n",
    "    print(f'DataFrame of shape {df.shape} created. Should be (n_months * 180 * 180, 8)')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start from scratch again\n",
    "\n",
    "PATH_df = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_data.pkl\"\n",
    "PATH_posterior_dict = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/generated/posterior_dict_36_calibration_20240613_165106.pkl\"\n",
    "\n",
    "# get the df from the pickle file in raw_data\n",
    "df = pd.read_pickle(PATH_df)\n",
    "\n",
    "# get the posterior_dict from the pickle file in generated\n",
    "with open(PATH_posterior_dict, 'rb') as f:\n",
    "    posterior_dict = pickle.load(f)\n",
    "\n",
    "month_range = 36 # but what even is it in the partion by now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the three lists from the posterior_dict\n",
    "posterior_list, posterior_list_class, out_of_sample_vol = posterior_dict['posterior_list'], posterior_dict['posterior_list_class'], posterior_dict['out_of_sample_vol'] # obviously there will be no out_of_sample_vol with true forecasting...\n",
    "\n",
    "# the posterior_list is a list of list of arrays\n",
    "# the first list is the number of draws from the posterior\n",
    "# the second list is the number of months in the forecast\n",
    "# the arrays are the forecasted volumes for size 3x180x180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe you can just use a modfication of this loop to make the df?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT!!!\n",
    "\n",
    "So, see if you can make a df from this - remember two things: \n",
    "- you don't have an out_of_sample_vol for true forecasts. \n",
    "- And out_of_sample_vol also don't have any features beyond history of violence..  \n",
    "\n",
    "So this is to a large extent a validation execice we need to see if we can:\n",
    "- turn it back into a df\n",
    "- get the same results as with the get_log_dict\n",
    "- Anf then figure out how to get the same resultes with the df of vol... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol):\n",
    "\n",
    "    \"\"\"Return a dictionary of metrics for the monthly out-of-sample predictions for W&B.\"\"\"\n",
    "\n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = i\n",
    "\n",
    "\n",
    "    #Fix in a sec when you see if it runs at all.... \n",
    "    for j in range(3): #(config.targets): # TARGETS IS & BUT THIS SHOULD BE 3!!!!!\n",
    "\n",
    "        y_score = mean_array[i,j,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[i,j,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[i,j,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[i,j,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        y_true = out_of_sample_vol[:,i,j,:,:].reshape(-1)  # nu 180x180 . dim 0 is time\n",
    "        y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        log_dict[f\"monthly/mean_squared_error{j}\"] = mse\n",
    "        log_dict[f\"monthly/average_precision_score{j}\"] = ap\n",
    "        log_dict[f\"monthly/roc_auc_score{j}\"] = auc\n",
    "        log_dict[f\"monthly/brier_score_loss{j}\"] = brier\n",
    "\n",
    "\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKAY! lets start with the thing from eval\n",
    "# Get mean and std\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "out_sample_month_list = [] # only used for pickle...\n",
    "ap_list = []\n",
    "mse_list = []\n",
    "auc_list = []\n",
    "brier_list = []\n",
    "\n",
    "#NEW\n",
    "log_dict_list = []\n",
    "\n",
    "for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim\n",
    "\n",
    "    #y_score = mean_array[i].reshape(-1) # make it 1d  # nu 180x180\n",
    "    #y_score_prob = mean_class_array[i].reshape(-1) # nu 180x180\n",
    "\n",
    "    # do not really know what to do with these yet.\n",
    "#    y_var = std_array[i].reshape(-1)  # nu 180x180\n",
    "#    y_var_prob = std_class_array[i].reshape(-1)  # nu 180x180\n",
    "\n",
    "#    y_true = out_of_sample_vol[:,i].reshape(-1)  # nu 180x180 . dim 0 is time\n",
    " #   y_true_binary = (y_true > 0) * 1\n",
    "#\n",
    "    # log the metrics to WandB - but why here? \n",
    "    log_dict = get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol)# so at least it gets reported sep.\n",
    "    log_dict_list.append(log_dict)\n",
    "\n",
    "    #wandb.log(log_dict)\n",
    "\n",
    "    # YOU KNOW THIS IS WRONG ALREADY!!!!\n",
    "\n",
    "#    # this could be a function in utils_wandb or in common_utils... \n",
    "#    mse = mean_squared_error(y_true, y_score)  \n",
    "#    ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "#    auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "#    brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "#\n",
    "#    out_sample_month_list.append(i) # only used for pickle...\n",
    "#    mse_list.append(mse)\n",
    "#    ap_list.append(ap) # add to list.\n",
    "#    auc_list.append(auc)\n",
    "#    brier_list.append(brier)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(log_dict_list, num_months, feature = 0):\n",
    "    \"\"\"\n",
    "    Plots MSE, Average Precision, ROC AUC, and Brier Score for each month from log_dict_list.\n",
    "\n",
    "    Args:\n",
    "        log_dict_list (list of dict): List of dictionaries with monthly metrics.\n",
    "        num_months (int): Number of months to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics for each month\n",
    "    mse_list = []\n",
    "    ap_list = []\n",
    "    auc_list = []\n",
    "    brier_list = []\n",
    "\n",
    "    # Iterate over the log_dict_list and extract the metrics\n",
    "    for i in range(num_months):\n",
    "        mse_list.append(log_dict_list[i][f'monthly/mean_squared_error{feature}'])\n",
    "        ap_list.append(log_dict_list[i][f'monthly/average_precision_score{feature}'])\n",
    "        auc_list.append(log_dict_list[i][f'monthly/roc_auc_score{feature}'])\n",
    "        brier_list.append(log_dict_list[i][f'monthly/brier_score_loss{feature}'])\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "    # Plot MSE\n",
    "    axs[0, 0].plot(range(1, len(mse_list) + 1), mse_list, marker='o', color='b', label='MSE')\n",
    "    axs[0, 0].set_title('Mean Squared Error')\n",
    "    axs[0, 0].set_xlabel('Month')\n",
    "    axs[0, 0].set_ylabel('MSE')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot Average Precision\n",
    "    axs[0, 1].plot(range(1, len(ap_list) + 1), ap_list, marker='o', color='g', label='Average Precision')\n",
    "    axs[0, 1].set_title('Average Precision Score')\n",
    "    axs[0, 1].set_xlabel('Month')\n",
    "    axs[0, 1].set_ylabel('AP Score')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # Plot ROC AUC\n",
    "    axs[1, 0].plot(range(1, len(auc_list) + 1), auc_list, marker='o', color='r', label='ROC AUC')\n",
    "    axs[1, 0].set_title('ROC AUC Score')\n",
    "    axs[1, 0].set_xlabel('Month')\n",
    "    axs[1, 0].set_ylabel('AUC Score')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Plot Brier Score\n",
    "    axs[1, 1].plot(range(1, len(brier_list) + 1), brier_list, marker='o', color='m', label='Brier Score')\n",
    "    axs[1, 1].set_title('Brier Score Loss')\n",
    "    axs[1, 1].set_xlabel('Month')\n",
    "    axs[1, 1].set_ylabel('Brier Score')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    # add a title\n",
    "    plt.suptitle(f'Metrics for Feature {feature} Over {num_months} Months', fontsize=16)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(log_dict_list, 36, 0)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(log_dict_list, 36, 1)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(log_dict_list, 36, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol):\n",
    "\n",
    "    \"\"\"Return a dictionary of metrics for the monthly out-of-sample predictions for W&B.\"\"\"\n",
    "\n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = i\n",
    "\n",
    "\n",
    "    #Fix in a sec when you see if it runs at all.... \n",
    "    for j in range(3): #(config.targets): # TARGETS IS & BUT THIS SHOULD BE 3!!!!!\n",
    "\n",
    "        y_score = mean_array[i,j,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[i,j,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[i,j,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[i,j,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        y_true = out_of_sample_vol[:,i,j,:,:].reshape(-1)  # nu 180x180 . dim 0 is time\n",
    "        y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        log_dict[f\"monthly/mean_squared_error{j}\"] = mse\n",
    "        log_dict[f\"monthly/average_precision_score{j}\"] = ap\n",
    "        log_dict[f\"monthly/roc_auc_score{j}\"] = auc\n",
    "        log_dict[f\"monthly/brier_score_loss{j}\"] = brier\n",
    "\n",
    "\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "# SHIT THAT WORKS!!!\n",
    "## start reading here!!! :\n",
    "\n",
    "This is the thing that need to be the fundament.\n",
    "First I need to see that this works with the storage_array or another way of retaining or retreaving pg_id and month_id\n",
    "One possible way to do this is by using the meta_tensor you have started on below\n",
    "An alternative would be to to make a full full_tensor. I.e one that contains both the 3 prime feature and the \"meta\" fatures\n",
    "Ones we have proven we can get month_id and pgm back we need to see a out-of-sampel solution where we go beyond what is in the df\n",
    "And then I think we need to go back streamline the evaluation process so it follows. WHile doing so I must\n",
    "    \n",
    "    - keep true forecasting firmly in the mind\n",
    "    - Think about what, if anything, can be abstracted out to common_utils for the sake of getting it right for the stepshifters - maybe wandb stuff\n",
    "    - And while ad it, make sure you use you new data class to store the monthly metrics\n",
    "    - The more you can absract out and make general, the simple it should be\n",
    "    - And while you add it, make sure to check if the partitions are the same as in paper. If they are, get the results down and finish paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start from scratch again\n",
    "\n",
    "PATH_df = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_df.pkl\"\n",
    "PATH_posterior_dict = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/generated/posterior_dict_36_calibration_20240613_165106.pkl\"\n",
    "\n",
    "# get the df from the pickle file in raw_data\n",
    "df = pd.read_pickle(PATH_df)\n",
    "\n",
    "# get the posterior_dict from the pickle file in generated\n",
    "with open(PATH_posterior_dict, 'rb') as f:\n",
    "    posterior_dict = pickle.load(f)\n",
    "\n",
    "month_range = 36\n",
    "\n",
    "# get the three lists from the posterior_dict - we make the out_of_sample_vol later\n",
    "posterior_list, posterior_list_class, _ = posterior_dict['posterior_list'], posterior_dict['posterior_list_class'], posterior_dict['out_of_sample_vol'] # obviously there will be no out_of_sample_vol with true forecasting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_full_tensor(views_vol): #, config, device):\n",
    "\n",
    "    \"\"\"Uses to get the features for the full tensor\n",
    "    Used for out-of-sample predictions for both evaluation and forecasting, depending on the run_type (partition). \n",
    "    The test tensor is of size 1 x config.time_steps x config.input_channels x 180 x 180.\"\"\"\n",
    "\n",
    "    ln_best_sb_idx = 5#config.first_feature_idx # 5 = ln_best_sb\n",
    "    last_feature_idx = ln_best_sb_idx + month_range #config.input_channels\n",
    "\n",
    "    print(f'views_vol shape {views_vol.shape}')\n",
    "\n",
    "    # THIS IS WHERE YOU LOOSE THE OTHE FEATURES!!!!\n",
    "    full_tensor = torch.tensor(views_vol).float().unsqueeze(dim=0).permute(0,1,4,2,3)[:, :, ln_best_sb_idx:last_feature_idx, :, :] \n",
    "\n",
    "    # Make a metadata tensor with evrything else\n",
    "    metadata_tensor = torch.tensor(views_vol).float().unsqueeze(dim=0).permute(0,1,4,2,3)[:, :, :ln_best_sb_idx, :, :]\n",
    "\n",
    "    print(f'full_tensor shape {full_tensor.shape}')\n",
    "\n",
    "    return full_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_vol = df_to_vol(df)\n",
    "views_vol = views_vol.copy() # why the fuck face this works I swear I have no idea\n",
    "\n",
    "print(views_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tensor = get_full_tensor(views_vol ) #, config, device) # better cal this evel tensor\n",
    "\n",
    "#out_of_sample_vol = full_tensor[:,-config.time_steps:,:,:,:].cpu().numpy() # From the test tensor get the out-of-sample time_steps. \n",
    "out_of_sample_vol = full_tensor[:,-month_range:,:,:,:] #.cpu().numpy() # From the test tensor get the out-of-sample time_steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKAY! lets start with the thing from eval\n",
    "# Get mean and std\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "out_sample_month_list = [] # only used for pickle...\n",
    "ap_list = []\n",
    "mse_list = []\n",
    "auc_list = []\n",
    "brier_list = []\n",
    "\n",
    "#NEW\n",
    "log_dict_list = []\n",
    "feature_dict_list = []\n",
    "\n",
    "for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim\n",
    "\n",
    "    # log the metrics to WandB - but why here? \n",
    "    #log_dict = get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol)# so at least it gets reported sep.\n",
    "    \n",
    "    \n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = i\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "\n",
    "    #Fix in a sec when you see if it runs at all.... \n",
    "    for j in range(3): #(config.targets): # TARGETS IS & BUT THIS SHOULD BE 3!!!!!\n",
    "\n",
    "        y_score = mean_array[i,j,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[i,j,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[i,j,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[i,j,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        y_true = out_of_sample_vol[:,i,j,:,:].reshape(-1)  # nu 180x180 . dim 0 is time     THE TRICK IS NOW TO USE A df -> vol and not out_of_sample_vol...\n",
    "        y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "        # data\n",
    "        feature_dict[f\"y_score{j}\"] = y_score\n",
    "        feature_dict[f\"y_score_prob{j}\"] = y_score_prob\n",
    "        feature_dict[f\"y_var{j}\"] = y_var\n",
    "        feature_dict[f\"y_var_prob{j}\"] = y_var_prob\n",
    "        feature_dict[f\"y_true{j}\"] = y_true\n",
    "        feature_dict[f\"y_true_binary{j}\"] = y_true_binary\n",
    "\n",
    "        # metrics\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        log_dict[f\"monthly/mean_squared_error{j}\"] = mse\n",
    "        log_dict[f\"monthly/average_precision_score{j}\"] = ap\n",
    "        log_dict[f\"monthly/roc_auc_score{j}\"] = auc\n",
    "        log_dict[f\"monthly/brier_score_loss{j}\"] = brier\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_dict_list.append(feature_dict)\n",
    "    log_dict_list.append(log_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = pd.DataFrame.from_dict(feature_dict_list).apply(pd.Series.explode)\n",
    "\n",
    "# reset the index and keep the old index as a column month\n",
    "df_test2 = df_test2.reset_index().rename(columns = {\"index\": \"month\"})\n",
    "\n",
    "df_test2 # we don't konw if the month order is correct yet... But I highly suspect that it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all datatyps in columns\n",
    "df_test2.dtypes\n",
    "\n",
    "# change all columns to float\n",
    "df_test2 = df_test2.astype(float)\n",
    "\n",
    "# check all datatyps in columns\n",
    "df_test2.dtypes\n",
    "\n",
    "# make the binary columns integers\n",
    "df_test2 = df_test2.astype({\"y_true_binary0\": int, \"y_true_binary1\": int, \"y_true_binary2\": int})\n",
    "\n",
    "# check all datatyps in columns\n",
    "df_test2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_precision_score(df_test2[\"y_true_binary0\"], df_test2[\"y_score_prob0\"]))\n",
    "print(average_precision_score(df_test2[\"y_true_binary1\"], df_test2[\"y_score_prob1\"]))\n",
    "print(average_precision_score(df_test2[\"y_true_binary2\"], df_test2[\"y_score_prob2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so this works... lest recreat the monthly metrics for all features and the plot from above\n",
    "\n",
    "def plot_metrics(df_test2, feature = 0):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots MSE, Average Precision, ROC AUC, and Brier Score for each month from log_dict_list.\n",
    "\n",
    "    Args:\n",
    "        log_dict_list (list of dict): List of dictionaries with monthly metrics.\n",
    "        num_months (int): Number of months to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics for each month\n",
    "    mse_list = []\n",
    "    ap_list = []\n",
    "    auc_list = []\n",
    "    brier_list = []\n",
    "\n",
    "\n",
    "    # Iterate over the log_dict_list and extract the metrics\n",
    "    for i in df_test2[\"month\"].unique():\n",
    "\n",
    "        y_score = df_test2[df_test2[\"month\"] == i][f\"y_score{feature}\"]\n",
    "        y_score_prob = df_test2[df_test2[\"month\"] == i][f\"y_score_prob{feature}\"]\n",
    "        y_true = df_test2[df_test2[\"month\"] == i][f\"y_true{feature}\"]\n",
    "        y_true_binary = df_test2[df_test2[\"month\"] == i][f\"y_true_binary{feature}\"]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        ap_list.append(ap)\n",
    "        auc_list.append(auc)\n",
    "        brier_list.append(brier)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "    # Plot MSE\n",
    "    axs[0, 0].plot(range(1, len(mse_list) + 1), mse_list, marker='o', color='b', label='MSE')\n",
    "    axs[0, 0].set_title('Mean Squared Error')\n",
    "    axs[0, 0].set_xlabel('Month')\n",
    "    axs[0, 0].set_ylabel('MSE')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot Average Precision\n",
    "    axs[0, 1].plot(range(1, len(ap_list) + 1), ap_list, marker='o', color='g', label='Average Precision')\n",
    "    axs[0, 1].set_title('Average Precision Score')\n",
    "    axs[0, 1].set_xlabel('Month')\n",
    "    axs[0, 1].set_ylabel('AP Score')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # Plot ROC AUC\n",
    "    axs[1, 0].plot(range(1, len(auc_list) + 1), auc_list, marker='o', color='r', label='ROC AUC')\n",
    "    axs[1, 0].set_title('ROC AUC Score')\n",
    "    axs[1, 0].set_xlabel('Month')\n",
    "    axs[1, 0].set_ylabel('AUC Score')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Plot Brier Score\n",
    "    axs[1, 1].plot(range(1, len(brier_list) + 1), brier_list, marker='o', color='m', label='Brier Score')\n",
    "    axs[1, 1].set_title('Brier Score Loss')\n",
    "    axs[1, 1].set_xlabel('Month')\n",
    "    axs[1, 1].set_ylabel('Brier Score')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    # add a title\n",
    "    plt.suptitle(f'Metrics for Feature {feature} Over {df_test2[\"month\"].max()} Months', fontsize=16)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 0)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 1)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.from_dict(log_dict_list)\n",
    "df_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shit that works is right above\n",
    "\n",
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trynna make the df this style\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "#std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "#std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "\n",
    "#dict_features = {}\n",
    "\n",
    "df_features = pd.DataFrame()\n",
    "\n",
    "for j in range(3):\n",
    "#for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim. you could use an actual month_id array here\n",
    "\n",
    "#    print(i)\n",
    "\n",
    "#    dict_features[j] = {}\n",
    "\n",
    "    y_score_list = []\n",
    "    y_score_prob_list = []\n",
    "    y_true_list = []\n",
    "    y_true_binary_list = []\n",
    "    month_list  = []\n",
    "\n",
    "    #for j in range(3): # 3 features\n",
    "    for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim\n",
    "\n",
    "        y_score_list.append(mean_array[i,j,:,:].reshape(-1)) # make it 1d  # nu 180x180\n",
    "        y_score_prob_list.append(mean_class_array[i,j,:,:].reshape(-1)) # nu 180x180\n",
    "        y_true_list.append(out_of_sample_vol[:,i,j,:,:].reshape(-1))  # nu 180x180 . dim 0 is time\n",
    "        y_true_binary_list.append((y_true > 0) * 1)\n",
    "\n",
    "        # month of same size as y_score_list\n",
    "\n",
    "        if j == 0: # only do this once\n",
    "            month_list.append(np.array([i]*len(y_score_list[i])))\n",
    "\n",
    "    # lets go straight to df\n",
    "    if j == 0:\n",
    "        df_features[f'month'] = np.concatenate(month_list)\n",
    "    \n",
    "    df_features[f'y_score_{j}'] = np.concatenate(y_score_list)\n",
    "    df_features[f'y_score_prob_{j}'] = np.concatenate(y_score_prob_list)\n",
    "    df_features[f'y_true_{j}'] = np.concatenate(y_true_list)\n",
    "    df_features[f'y_true_binary_{j}'] = np.concatenate(y_true_binary_list)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#    dict_features[j][\"y_score\"] = y_score_list\n",
    "#   dict_features[j][\"y_score_prob\"] = y_score_prob_list\n",
    "#    dict_features[j][\"y_true\"] = y_true_list\n",
    "#    dict_features[j][\"y_true_binary\"] = y_true_binary_list\n",
    "#    dict_features[j][\"month\"] = month_list\n",
    "\n",
    "    # and now month so just i in and array the right size\n",
    "    #dict_features[j]['month'] = np.array([i]*len(y_score_list[0])) # just make it the right size\n",
    "\n",
    "\n",
    "# now the dict to df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(df_features[df_features['month'] == 0]['y_true_binary_1'], df_features[df_features['month'] == 0]['y_score_prob_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firts feature 0,1, or 2. them the output type - y_score, y_score_prob, y_true, y_true_binary. then the month and then the 180x180 array\n",
    "\n",
    "month_array = np.array(dict_features[0][\"month\"]).reshape(-1)\n",
    "y_score_array_0 = np.array(dict_features[0][\"y_score\"]).reshape(-1)\n",
    "y_score_prob_array_0 = np.array(dict_features[0][\"y_score_prob\"]).reshape(-1)\n",
    "y_true_array_0 = np.array(dict_features[0][\"y_true\"]).reshape(-1)\n",
    "y_true_binary_array_0 = np.array(dict_features[0][\"y_true_binary\"]).reshape(-1)\n",
    "\n",
    "y_score_array_1 = np.array(dict_features[1][\"y_score\"]).reshape(-1)\n",
    "y_score_prob_array_1 = np.array(dict_features[1][\"y_score_prob\"]).reshape(-1)\n",
    "y_true_array_1 = np.array(dict_features[1][\"y_true\"]).reshape(-1)\n",
    "y_true_binary_array_1 = np.array(dict_features[1][\"y_true_binary\"]).reshape(-1)\n",
    "\n",
    "y_score_array_2 = np.array(dict_features[2][\"y_score\"]).reshape(-1)\n",
    "y_score_prob_array_2 = np.array(dict_features[2][\"y_score_prob\"]).reshape(-1)\n",
    "y_true_array_2 = np.array(dict_features[2][\"y_true\"]).reshape(-1)\n",
    "y_true_binary_array_2 = np.array(dict_features[2][\"y_true_binary\"]).reshape(-1)\n",
    "\n",
    "df_features = pd.DataFrame({\n",
    "    'month_id': month_array,\n",
    "    'y_score_0': y_score_array_0,\n",
    "    'y_score_prob_0': y_score_prob_array_0,\n",
    "    'y_true_0': y_true_array_0,\n",
    "    'y_true_binary_0': y_true_binary_array_0,\n",
    "    'y_score_1': y_score_array_1,\n",
    "    'y_score_prob_1': y_score_prob_array_1,\n",
    "    'y_true_1': y_true_array_1,\n",
    "    'y_true_binary_1': y_true_binary_array_1,\n",
    "    'y_score_2': y_score_array_2,\n",
    "    'y_score_prob_2': y_score_prob_array_2,\n",
    "    'y_true_2': y_true_array_2,\n",
    "    'y_true_binary_2': y_true_binary_array_2,\n",
    "})\n",
    "\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(df_features[\"y_true_binary_0\"], df_features[\"y_score_prob_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the monthly metrics for each feature in the df\n",
    "\n",
    "for i in df_features[\"month_id\"].unique():\n",
    "    print(i)\n",
    "    df_month = df_features[df_features[\"month_id\"] == i]\n",
    "\n",
    "    for j in range(3):\n",
    "        y_score = df_month[f\"y_score_{j}\"]\n",
    "        y_score_prob = df_month[f\"y_score_prob_{j}\"]\n",
    "        y_true = df_month[f\"y_true_{j}\"]\n",
    "        y_true_binary = df_month[f\"y_true_binary_{j}\"]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        print(f\"Feature {j} - MSE: {mse}, AP: {ap}, AUC: {auc}, Brier: {brier}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the posterior_list to a numpy array of shape (n_draws, n_months, 3, 180, 180)\n",
    "posterior_array = np.array(posterior_list)\n",
    "print(posterior_array.shape)\n",
    "\n",
    "# revesr the order of the months\n",
    "posterior_array = np.flip(posterior_array, axis = 1)\n",
    "\n",
    "# for now we can take the mean (later hdi) of the posterior_array as a point estimate\n",
    "posterior_mean = np.mean(posterior_array, axis=0)\n",
    "print(posterior_mean.shape)\n",
    "\n",
    "# reverse the order of the months\n",
    "#posterior_mean = np.flip(posterior_mean, axis = 0)\n",
    "\n",
    "# reshape the posterior_mean to be similar to the vol\n",
    "posterior_mean = np.transpose(posterior_mean, (0, 2, 3, 1))\n",
    "print(posterior_mean.shape)\n",
    "\n",
    "# reverse the order of the months\n",
    "#posterior_mean = np.flip(posterior_mean, axis = 0)\n",
    "\n",
    "# and for class\n",
    "posterior_class_array = np.array(posterior_list_class)\n",
    "print(posterior_class_array.shape)\n",
    "\n",
    "# revesr the order of the months\n",
    "#posterior_class_array = np.flip(posterior_class_array, axis = 1)\n",
    "\n",
    "# for now we can take the mean (later hdi) of the posterior_array as a point estimate\n",
    "posterior_class_mean = np.mean(posterior_class_array, axis=0)\n",
    "print(posterior_class_mean.shape)\n",
    "\n",
    "# reverse the order of the months   \n",
    "#posterior_class_mean = np.flip(posterior_class_mean, axis = 0)\n",
    "\n",
    "# reshape the posterior_mean to be similar to the vol\n",
    "posterior_class_mean = np.transpose(posterior_class_mean, (0, 2, 3, 1))\n",
    "print(posterior_class_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the posterior_mean and the posterior_class_mean\n",
    "posterior_all_mean = np.concatenate([posterior_mean, posterior_class_mean], axis=-1)\n",
    "\n",
    "# reverse the order of the months\n",
    "#posterior_all_mean = np.flip(posterior_all_mean, axis = 0)\n",
    "\n",
    "print(posterior_all_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_storage_vol = make_forecast_storage_vol(df, month_range=month_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vol = merge_vol(forecast_storage_vol, posterior_all_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we merge with the out_of_sample_vol instead?\n",
    "\n",
    "# remove batch dimension from out_of_sample_vol\n",
    "out_of_sample_vol_nb = np.squeeze(out_of_sample_vol, axis=0)\n",
    "\n",
    "# reshape the out_of_sample_vol_nb to be similar to the vol\n",
    "out_of_sample_vol_nb = np.transpose(out_of_sample_vol_nb, (0, 2, 3, 1))\n",
    "\n",
    "#merge the out_of_sample_vol_nb with the forecast_storage_vol\n",
    "new_vol_oos = merge_vol(forecast_storage_vol, out_of_sample_vol_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_vol(new_vol, 3)\n",
    "plot_vol(new_vol_oos, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = vol_to_df_new(new_vol)\n",
    "df_new_oos = vol_to_df_oos(new_vol_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['month_id'].max()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the correctly the df_new have all months_ids moved forward. But since the evaluation set is here part of the original df, I'll just push it back to the original month_ids\n",
    "df_new[\"month_id\"] = df_new[\"month_id\"] - month_range\n",
    "print(df_new[\"month_id\"].max())\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now subset the df to only include the last month_range months\n",
    "month_max = df[\"month_id\"].max()\n",
    "month_array = np.arange(month_max - month_range +1, month_max+1)\n",
    "df_sub = df[df['month_id'].isin(month_array)]\n",
    "\n",
    "print(df_sub['month_id'].max())\n",
    "\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does that compare the df_new_oos\n",
    "\n",
    "# you need to take the month_id back to the oriringal month_id\n",
    "df_new_oos[\"month_id\"] = df_new_oos[\"month_id\"] - month_range\n",
    "df_new_oos[\"month_id\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df (df_merged) which is df_new but with ln_sb_best, ln_ns_best, ln_os_best from df_sub merged by pg_id and month_id\n",
    "\n",
    "# real\n",
    "#df_merged = df_new.merge(df_sub[['pg_id', 'month_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']], on=['pg_id', 'month_id'], how='left')\n",
    "\n",
    "# oos\n",
    "df_merged = df_new.merge(df_new_oos[['pg_id', 'month_id', 'ln_sb_best_oos', 'ln_ns_best_oos', 'ln_os_best_oos']], on=['pg_id', 'month_id'], how='left')\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now a correlation plot between the ln_sb_best, ln_ns_best, ln_os_best and ln_sb_pred, ln_ns_pred, ln_os_pred and the same for the probas\n",
    "#df_merged[['ln_sb_best', 'ln_ns_best', 'ln_os_best', 'ln_sb_pred', 'ln_ns_pred', 'ln_os_pred', 'proba_sb_pred', 'proba_ns_pred', 'proba_os_pred']].corr()\n",
    "\n",
    "df_merged[['ln_sb_best_oos', 'ln_ns_best_oos', 'ln_os_best_oos', 'ln_sb_pred', 'ln_ns_pred', 'ln_os_pred', 'proba_sb_pred', 'proba_ns_pred', 'proba_os_pred']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now a correlation plot between the ln_sb_best, ln_ns_best, ln_os_best and ln_sb_pred, ln_ns_pred, ln_os_pred and the same for the probas\n",
    "df_merged[['ln_sb_best', 'ln_ns_best', 'ln_os_best', 'ln_sb_pred', 'ln_ns_pred', 'ln_os_pred', 'proba_sb_pred', 'proba_ns_pred', 'proba_os_pred']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# binarize the ln's\n",
    "# class_sb_best = df_merged['ln_sb_best'] > 0\n",
    "# class_ns_best = df_merged['ln_ns_best'] > 0\n",
    "# class_os_best = df_merged['ln_os_best'] > 0\n",
    "\n",
    "\n",
    "# binarize the ln's\n",
    "class_sb_best = df_merged['ln_sb_best_oos'] > 0\n",
    "class_ns_best = df_merged['ln_ns_best_oos'] > 0\n",
    "class_os_best = df_merged['ln_os_best_oos'] > 0\n",
    "\n",
    "# get the auroc for the probas\n",
    "auroc_sb = roc_auc_score(class_sb_best, df_merged['proba_sb_pred'])\n",
    "auroc_ns = roc_auc_score(class_ns_best, df_merged['proba_ns_pred'])\n",
    "auroc_os = roc_auc_score(class_os_best, df_merged['proba_os_pred'])\n",
    "\n",
    "# get the average precision for the probas\n",
    "ap_sb = average_precision_score(class_sb_best, df_merged['proba_sb_pred'])\n",
    "ap_ns = average_precision_score(class_ns_best, df_merged['proba_ns_pred'])\n",
    "ap_os = average_precision_score(class_os_best, df_merged['proba_os_pred'])\n",
    "\n",
    "# get the brier score for the probas\n",
    "brier_sb = brier_score_loss(class_sb_best, df_merged['proba_sb_pred'])\n",
    "brier_ns = brier_score_loss(class_ns_best, df_merged['proba_ns_pred'])\n",
    "brier_os = brier_score_loss(class_os_best, df_merged['proba_os_pred'])\n",
    "\n",
    "# get the mse for the ln's\n",
    "mse_sb = mean_squared_error(df_merged['ln_sb_best_oos'], df_merged['ln_sb_pred'])\n",
    "mse_ns = mean_squared_error(df_merged['ln_ns_best_oos'], df_merged['ln_ns_pred'])\n",
    "mse_os = mean_squared_error(df_merged['ln_os_best_oos'], df_merged['ln_os_pred'])\n",
    "\n",
    "# create a nice table with all the results\n",
    "results = pd.DataFrame({\n",
    "    'auroc': [auroc_sb, auroc_ns, auroc_os],\n",
    "    'ap': [ap_sb, ap_ns, ap_os],\n",
    "    'brier': [brier_sb, brier_ns, brier_os],\n",
    "    'mse': [mse_sb, mse_ns, mse_os]\n",
    "}, index=['sb', 'ns', 'os'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you are not even sure if the querysets are the same... Should be but are you 100%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how that corropsonds to is we use the out_of_sample_vol\n",
    "\n",
    "# remove the batch dimension\n",
    "out_of_sample_vol_no_batch = out_of_sample_vol[0]\n",
    "\n",
    "# reshape the out_of_sample_vol_no_batch to be similar to the vol\n",
    "out_of_sample_vol_no_batch = np.transpose(out_of_sample_vol_no_batch, (0, 2, 3, 1))\n",
    "print(out_of_sample_vol_no_batch.shape)\n",
    "\n",
    "# now compare this to the 48 month of the vol\n",
    "vol_3_36 = vol[-month_range:, :, :, 5:]\n",
    "print(vol_3_36.shape)\n",
    "\n",
    "# compare the two\n",
    "print(np.array_equal(out_of_sample_vol_no_batch, vol_3_36))\n",
    "\n",
    "# but are they near?\n",
    "print(np.allclose(out_of_sample_vol_no_batch, vol_3_36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but is there any correlation between the out_of_sample_vol and the vol_3_48?\n",
    "\n",
    "# get the corr between the two arrays\n",
    "print(np.corrcoef(out_of_sample_vol_no_batch.flatten(), vol_3_36.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is we forst sum each to see the total sum of the arrays and then substrat one from the other to see what the difference is\n",
    "print(np.sum(out_of_sample_vol_no_batch))\n",
    "print(np.sum(vol_3_36))\n",
    "\n",
    "print(np.sum(out_of_sample_vol_no_batch) - np.sum(vol_3_36)) # Just a rounding error it seems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot placehodlers for the two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now month specific metrics..\n",
    "\n",
    "# first create the list to store the individaul monthly metrics\n",
    "\n",
    "#auroc\n",
    "auroc_sb_list = []\n",
    "auroc_ns_list = []\n",
    "auroc_os_list = []\n",
    "\n",
    "#ap\n",
    "ap_sb_list = []\n",
    "ap_ns_list = []\n",
    "ap_os_list = []\n",
    "\n",
    "#brier\n",
    "brier_sb_list = []\n",
    "brier_ns_list = []\n",
    "brier_os_list = []\n",
    "\n",
    "#mse\n",
    "mse_sb_list = []\n",
    "mse_ns_list = []\n",
    "mse_os_list = []\n",
    "\n",
    "month_array = df_merged['month_id'].unique()\n",
    "#print(month_array)\n",
    "\n",
    "# loop through the months\n",
    "for i in month_array:\n",
    "    #print(i)\n",
    "\n",
    "    # subset the df_merged to only include the month\n",
    "    df_sub = df_merged[df_merged['month_id'] == i]\n",
    "\n",
    "    # binarize the ln's for auroc, ap and brier\n",
    "    #class_sb_best = df_sub['ln_sb_best'] > 0\n",
    "    #class_ns_best = df_sub['ln_ns_best'] > 0\n",
    "    #class_os_best = df_sub['ln_os_best'] > 0\n",
    "\n",
    "    # binarize the ln's for auroc, ap and brier\n",
    "    class_sb_best = df_sub['ln_sb_best_oos'] > 0\n",
    "    class_ns_best = df_sub['ln_ns_best_oos'] > 0\n",
    "    class_os_best = df_sub['ln_os_best_oos'] > 0\n",
    "\n",
    "    # get the auroc for the probas and append to the list\n",
    "    auroc_sb_list.append(roc_auc_score(class_sb_best, df_sub['proba_sb_pred']))\n",
    "    auroc_ns_list.append(roc_auc_score(class_ns_best, df_sub['proba_ns_pred']))\n",
    "    auroc_os_list.append(roc_auc_score(class_os_best, df_sub['proba_os_pred']))\n",
    "\n",
    "    # get the average precision for the probas and append to the list\n",
    "    ap_sb_list.append(average_precision_score(class_sb_best, df_sub['proba_sb_pred']))\n",
    "    ap_ns_list.append(average_precision_score(class_ns_best, df_sub['proba_ns_pred']))\n",
    "    ap_os_list.append(average_precision_score(class_os_best, df_sub['proba_os_pred']))\n",
    "\n",
    "    # get the brier score for the probas and append to the list\n",
    "    brier_sb_list.append(brier_score_loss(class_sb_best, df_sub['proba_sb_pred']))\n",
    "    brier_ns_list.append(brier_score_loss(class_ns_best, df_sub['proba_ns_pred']))\n",
    "    brier_os_list.append(brier_score_loss(class_os_best, df_sub['proba_os_pred']))\n",
    "\n",
    "    # get the mse for the ln's and append to the list\n",
    "    #mse_sb_list.append(mean_squared_error(df_sub['ln_sb_best'], df_sub['ln_sb_pred']))\n",
    "    #mse_ns_list.append(mean_squared_error(df_sub['ln_ns_best'], df_sub['ln_ns_pred']))\n",
    "    #mse_os_list.append(mean_squared_error(df_sub['ln_os_best'], df_sub['ln_os_pred']))\n",
    "\n",
    "    # get the mse for the ln's and append to the list\n",
    "    mse_sb_list.append(mean_squared_error(df_sub['ln_sb_best_oos'], df_sub['ln_sb_pred']))\n",
    "    mse_ns_list.append(mean_squared_error(df_sub['ln_ns_best_oos'], df_sub['ln_ns_pred']))\n",
    "    mse_os_list.append(mean_squared_error(df_sub['ln_os_best_oos'], df_sub['ln_os_pred']))\n",
    "\n",
    "# create a nice table with all the results\n",
    "results_month = pd.DataFrame({\n",
    "    'auroc_sb': auroc_sb_list,\n",
    "    'auroc_ns': auroc_ns_list,\n",
    "    'auroc_os': auroc_os_list,\n",
    "    'ap_sb': ap_sb_list,\n",
    "    'ap_ns': ap_ns_list,\n",
    "    'ap_os': ap_os_list,\n",
    "    'brier_sb': brier_sb_list,\n",
    "    'brier_ns': brier_ns_list,\n",
    "    'brier_os': brier_os_list,\n",
    "    'mse_sb': mse_sb_list,\n",
    "    'mse_ns': mse_ns_list,\n",
    "    'mse_os': mse_os_list,\n",
    "    'moonth_id': month_array\n",
    "}, index=month_array)\n",
    "\n",
    "results_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 3 aps\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(results_month['moonth_id'], results_month['ap_sb'], label='sb')\n",
    "plt.plot(results_month['moonth_id'], results_month['ap_ns'], label='ns')\n",
    "plt.plot(results_month['moonth_id'], results_month['ap_os'], label='os')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# they are recognizably when compared to the WandB but distintly worse... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So currently I can know if this is right... You need a posterior dict from fimbultuhul which is also evaluation on wieghts and biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
