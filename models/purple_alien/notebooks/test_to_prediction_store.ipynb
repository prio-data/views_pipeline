{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpd_2023 environment\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch    \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# now get AUROC and average precision for the probas and mse for the ln's\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, mean_squared_error, brier_score_loss\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# set path to the utils\n",
    "PATH = Path(\"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/notebooks/test_to_prediction_store.ipynb\")\n",
    "sys.path.insert(0, str(Path(*[i for i in PATH.parts[:PATH.parts.index(\"views_pipeline\")+1]]) / \"common_utils\")) # PATH_COMMON_UTILS  \n",
    "from set_path import setup_project_paths, setup_data_paths\n",
    "setup_project_paths(PATH)\n",
    "\n",
    "# now import the local functions\n",
    "from utils_df_to_vol_conversion import df_to_vol, df_vol_conversion_test, plot_vol\n",
    "from utils_wandb import generate_wandb_log_dict \n",
    "from utils_evaluation_metrics import EvaluationMetrics\n",
    "from utils_model_outputs import ModelOutputs\n",
    "from utils_hydranet_outputs import output_to_df, evaluation_to_df, plot_metrics\n",
    "from utils import get_full_tensor\n",
    "\n",
    "# from somehwere imort get_full_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions \n",
    "We start with all well functioning well documented functions - note where the \"live\" and if anything needs to be adjusted for the if/when the function goes into a script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should move stuff to common_utils now if it is done... \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that you volumn can be loaded and is correct.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_df = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_df.pkl\"\n",
    "\n",
    "# get the df from the pickle file in raw_data\n",
    "df = pd.read_pickle(PATH_df)\n",
    "\n",
    "# turn the df1 into a volume\n",
    "vol = df_to_vol(df)\n",
    "\n",
    "df_vol_conversion_test(df, vol)\n",
    "\n",
    "plot_vol(vol, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then figure out the thing below with doing stuff without the ourput tensor... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "# Stuff THAT WORKS!!!\n",
    "\n",
    "Now it is basically about:\n",
    "- take the bit loop below, and make sure it works for both eval and forecasing using the meta thing\n",
    "- Also there should be a simple way to turn as out_put_dict into a metric dict... \n",
    "    - Can you think future output drift detection into this or is that completely seperate?\n",
    "- Then package everything nice and neat\n",
    "- Distribute to relevant/appropiate scripts\n",
    "- Make sure to ubdate the eval rutine to corrospond\n",
    "- Then implement the forecasting rutine... Finally. \n",
    "\n",
    "\n",
    "## old notes\n",
    "\n",
    "## start reading here!!! :\n",
    "\n",
    "This is the thing that need to be the fundament.\n",
    "First I need to see that this works with the storage_array or another way of retaining or retreaving pg_id and month_id\n",
    "One possible way to do this is by using the meta_tensor you have started on below\n",
    "An alternative would be to to make a full full_tensor. I.e one that contains both the 3 prime feature and the \"meta\" fatures\n",
    "Ones we have proven we can get month_id and pgm back we need to see a out-of-sampel solution where we go beyond what is in the df\n",
    "And then I think we need to go back streamline the evaluation process so it follows. WHile doing so I must\n",
    "    \n",
    "- keep true forecasting firmly in the mind\n",
    "- Think about what, if anything, can be abstracted out to common_utils for the sake of getting it right for the stepshifters - maybe wandb stuff\n",
    "- And while ad it, make sure you use you new data class to store the monthly metrics -------------------------------------------------This now--------------------------\n",
    "- The more you can absract out and make general, the simple it should be\n",
    "- And while you add it, make sure to check if the partitions are the same as in paper. If they are, get the results down and finish paper.\n",
    "\n",
    "\n",
    "# WHAT YOU ARE REALLY DOING:\n",
    "- Aligning evaluation with forecasting to make a common appraoch\n",
    "- MAke sure that logging and uploading (wandb) of metrics is consice and consitant\n",
    "- Make sure that logging and uplaoding (predicion store and other?) os out outs from all partitions - whether true forecast or not - is consice and consistent\n",
    "- You might want a data class thing of outputs (like the one uo have ofr metrics). Generel for all models forecsating conflict\n",
    "- You want to make everythign as generel/abstract as possible - especially WandB stuff!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_posterior_dict = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/generated/posterior_dict_36_calibration_20240613_165106.pkl\"\n",
    "\n",
    "# get the posterior_dict from the pickle file in generated\n",
    "with open(PATH_posterior_dict, 'rb') as f:\n",
    "    posterior_dict = pickle.load(f)\n",
    "\n",
    "month_range = 36 # MAGIC NUMBER ALERT - this is the number of months in the future we are forecasting\n",
    "\n",
    "# get the three lists from the posterior_dict - we make the out_of_sample_vol later\n",
    "posterior_list, posterior_list_class, _ = posterior_dict['posterior_list'], posterior_dict['posterior_list_class'], posterior_dict['out_of_sample_vol'] # obviously there will be no out_of_sample_vol with true forecasting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the views_vol from the df\n",
    "views_vol = df_to_vol(df)\n",
    "views_vol = views_vol.copy() # why the fuck face this works I swear I have no idea\n",
    "\n",
    "# get the full_tensor and the new meta_data:tendor\n",
    "full_tensor, metadata_tensor = get_full_tensor(views_vol ) #, config, device) # better cal this evel tensor\n",
    "print(views_vol.shape)\n",
    "\n",
    "# Get the out of sample vol and the out of sampele meta_vol <---------------------------- THIS IS CENTRAL, BECAUSE TO REAL FORECASTING TO WORK YOU JUST NEED TO GEN A SYNTH out_of_sample_meta_vol\n",
    "out_of_sample_vol = full_tensor[:,-month_range:,:,:,:] #.cpu().numpy() # From the test tensor get the out-of-sample time_steps.\n",
    "out_of_sample_meta_vol = metadata_tensor[:,-month_range:,:,:,:]\n",
    "\n",
    "print(out_of_sample_vol.shape)\n",
    "print(out_of_sample_meta_vol.shape)\n",
    "\n",
    "# Merge full_tensor and metadata_tensor along the feature dimension - just a test\n",
    "full_t_new = np.concatenate((out_of_sample_meta_vol, out_of_sample_vol), axis=2)\n",
    "\n",
    "print(f'Shape of merged tensor: {full_t_new.shape}') # lets trust that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 36\n",
    "eval = True\n",
    "\n",
    "# bad name if eval is an arguement.... evaluate_andor_forecast_posterior\n",
    "\n",
    "#def evaluate_posterior(model, views_vol, config, device): # is eval in config?\n",
    "\n",
    "#posterior_list, posterior_list_class, out_of_sample_vol, full_tensor, metadata_tensor = sample_posterior(model, views_vol, config, device)\n",
    "\n",
    "\n",
    "if eval:\n",
    "    dict_of_eval_dicts = {}\n",
    "    dict_of_eval_dicts = {k: EvaluationMetrics.make_evaluation_dict(steps=steps) for k in [\"sb\", \"ns\", \"os\"]}\n",
    "\n",
    "dict_of_outputs_dicts = {}\n",
    "dict_of_outputs_dicts = {k: ModelOutputs.make_output_dict(steps=steps) for k in [\"sb\", \"ns\", \"os\"]}\n",
    "\n",
    "# Get mean and std\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "#NEW\n",
    "log_dict_list = []\n",
    "#feature_dict_list = []\n",
    "\n",
    "for t in range(mean_array.shape[0]): #  0 of mean array is the temporal dim    \n",
    "    \n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = t +1 # 1 indexed, bc the first step is 1 month ahead\n",
    "\n",
    "    for i, j in enumerate(dict_of_eval_dicts.keys()): # this is the same as the above but with the dict keys\n",
    "\n",
    "        step = f\"step{str(t+1).zfill(2)}\"\n",
    "        \n",
    "        # get the scores\n",
    "        y_score = mean_array[t,i,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[t,i,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[t,i,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[t,i,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        # see this is the out of sample vol - fine for evaluation but not for forecasting\n",
    "        # but also the place where you get the pgm.. \n",
    "\n",
    "        if eval:\n",
    "            y_true = out_of_sample_vol[:,t,i,:,:].numpy().reshape(-1)  # nu 180x180 . dim 0 is time     THE TRICK IS NOW TO USE A df -> vol and not out_of_sample_vol...\n",
    "            y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "            # in theorty you could just use the metadata tensor to get pg and c id here\n",
    "            pg_id = out_of_sample_meta_vol[:,t,0,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 0 is pg_id\n",
    "            c_id = out_of_sample_meta_vol[:,t,4,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 4 is c_id\n",
    "            month_id = out_of_sample_meta_vol[:,t,3,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 3 is month_id\n",
    "\n",
    "            dict_of_outputs_dicts[j][step].y_true = y_true\n",
    "            dict_of_outputs_dicts[j][step].y_true_binary = y_true_binary\n",
    "\n",
    "        else: # you need to make sure this works for forecasting\n",
    "            # in theorty you could just use the metadata tensor to get pg and c id here\n",
    "            pg_id = out_of_sample_meta_vol[:,t,0,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 0 is pg_id\n",
    "            c_id = out_of_sample_meta_vol[:,t,4,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 4 is c_id\n",
    "            month_id = out_of_sample_meta_vol[:,t,3,:,:].numpy().reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 3 is month_id\n",
    "\n",
    "\n",
    "        dict_of_outputs_dicts[j][step].y_score = y_score\n",
    "        dict_of_outputs_dicts[j][step].y_score_prob = y_score_prob\n",
    "        dict_of_outputs_dicts[j][step].y_var = y_var\n",
    "        dict_of_outputs_dicts[j][step].y_var_prob = y_var_prob\n",
    "\n",
    "        dict_of_outputs_dicts[j][step].pg_id = pg_id # in theory this should be in the right order\n",
    "        dict_of_outputs_dicts[j][step].c_id = c_id # in theory this should be in the right order\n",
    "        dict_of_outputs_dicts[j][step].step = t +1 # 1 indexed, bc the first step is 1 month ahead\n",
    "        dict_of_outputs_dicts[j][step].month_id = month_id\n",
    "\n",
    "        if eval:   \n",
    "\n",
    "            dict_of_eval_dicts[j][step].MSE = mean_squared_error(y_true, y_score)\n",
    "            dict_of_eval_dicts[j][step].AP = average_precision_score(y_true_binary, y_score_prob)\n",
    "            dict_of_eval_dicts[j][step].AUC = roc_auc_score(y_true_binary, y_score_prob)\n",
    "            dict_of_eval_dicts[j][step].Brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "            log_dict = generate_wandb_log_dict(log_dict, dict_of_eval_dicts, j, step)\n",
    "\n",
    "    if eval:\n",
    "        log_dict_list.append(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_outputs_dicts['sb']['step01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sb = ModelOutputs.output_dict_to_dataframe(dict_of_outputs_dicts[\"sb\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex the df_sb but keep the old index ad a column month_out_of_sample\n",
    "df_sb[\"month_out_of_sample\"] = df_sb.index +1\n",
    "df_sb = df_sb.reset_index(drop=True)\n",
    "df_sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = output_to_df(dict_of_outputs_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_precision_score(df_full[\"y_true_binary_sb\"], df_full[\"y_score_prob_sb\"]))\n",
    "print(average_precision_score(df_full[\"y_true_binary_ns\"], df_full[\"y_score_prob_ns\"]))\n",
    "print(average_precision_score(df_full[\"y_true_binary_os\"], df_full[\"y_score_prob_os\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[['pg_id', 'c_id', 'step', 'month_id', 'y_true_sb', 'y_score_sb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[df_full['pg_id'] == 190494][['pg_id', 'c_id', 'step', 'month_id', 'y_true_sb', 'y_score_sb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "plot_metrics(df_full, \"sb\")\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_full, \"ns\")\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_full, \"os\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = evaluation_to_df(dict_of_eval_dicts)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now forecasting storage test\n",
    "Now we test how we can make a storage array for the 4D forcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_true_forecasting import generate_fake_vol, make_forecast_storage_vol, merge_vol, check_vol_equal, plot_vol_comparison, check_month_id_consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_range = 36\n",
    "\n",
    "vol_fake = generate_fake_vol(vol, month_range=month_range)\n",
    "\n",
    "#print shape of vol_fake\n",
    "print(vol_fake.shape)\n",
    "\n",
    "# make the forecast storage volume\n",
    "forecast_storage_vol = make_forecast_storage_vol(month_range=month_range, to_tensor=False)\n",
    "\n",
    "#print shape of forecast_storage_vol\n",
    "print(forecast_storage_vol.shape)\n",
    "\n",
    "# merge the forecast storage volume with the forecast volume\n",
    "new_vol = merge_vol(forecast_storage_vol, vol_fake)\n",
    "\n",
    "#print shape of full_vol\n",
    "print(new_vol.shape)\n",
    "\n",
    "# Check that the full vol is equal to the original vol (sliced correctly)\n",
    "check_vol_equal(vol, new_vol)\n",
    "\n",
    "# plot the volume slices\n",
    "# plot_vol_comparison(vol, new_vol, month_range=month_range) # works and cornfirms that the two volumes are the same (except for month_id, which is expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the volume slices\n",
    "plot_vol(forecast_storage_vol, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the forecasting loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_posterior(model, views_vol, df, config, device):\n",
    "#def forecast_posterior(df, config):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrive true forecasts form sample_posterior and generate comprehensive DataFrame and volume representations of these forecasts.\n",
    "\n",
    "    This function handles posterior predictions for multiple features over time, calculates mean and standard deviations,\n",
    "    and compiles these metrics along with metadata into a DataFrame suitable for evaluation or further analysis.\n",
    "    Additionally, it constructs a volume for visualizations or plotting purposes.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model used to generate posterior predictions.\n",
    "        df (pd.DataFrame): DataFrame containing the initial data for generating forecast storage volume (month_id, pg_id, c_id, col, row).\n",
    "        config (dict): Configuration dictionary with model parameters, including 'time_steps' and 'month_range'. BAD TERMONOLOGY!!!!\n",
    "        device (torch.device): Device to run model computations (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, np.ndarray]: \n",
    "            - DataFrame containing processed model outputs with scores, variances, and metadata.\n",
    "            - 4D volume array suitable for testing and plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    # posterior_list, posterior_list_class, _, _, _, _ = sample_posterior(model, views_vol, config, device) <--------------------------\n",
    "\n",
    "    month_range = config['time_steps']\n",
    "\n",
    "    # storage volume for forecasts\n",
    "    forecast_storage_vol = make_forecast_storage_vol(month_range = month_range, to_tensor=True) # CONFIG\n",
    "\n",
    "    # Initialize dictionary to store outputs for different features\n",
    "    dict_of_outputs_dicts = {k: ModelOutputs.make_output_dict(steps = month_range) for k in [\"sb\", \"ns\", \"os\"]} # CONFIG - step is month_range... BAD NAME!\n",
    " \n",
    "    # Calculate mean and standard deviation for posterior predictions and class probabilities\n",
    "    mean_array = np.array(posterior_list).mean(axis=0)\n",
    "    std_array = np.array(posterior_list).std(axis=0)\n",
    "    mean_class_array = np.array(posterior_list_class).mean(axis=0)\n",
    "    std_class_array = np.array(posterior_list_class).std(axis=0)\n",
    "\n",
    "    for t in range(mean_array.shape[0]):  # Iterate over time steps\n",
    "        for i, feature_key in enumerate(dict_of_outputs_dicts.keys()):  # Iterate over feature keys ('sb', 'ns', 'os')\n",
    "            step = f\"step{str(t + 1).zfill(2)}\"\n",
    "\n",
    "            # Extract scores and variances for each feature\n",
    "            y_score = mean_array[t, i, :, :].reshape(-1)\n",
    "            y_score_prob = mean_class_array[t, i, :, :].reshape(-1)\n",
    "            y_var = std_array[t, i, :, :].reshape(-1)\n",
    "            y_var_prob = std_class_array[t, i, :, :].reshape(-1)\n",
    "\n",
    "            # Extract metadata: pg_id, c_id, month_id from the forecast storage volume\n",
    "            pg_id = forecast_storage_vol[:, t, 0, :, :].reshape(-1)\n",
    "            c_id = forecast_storage_vol[:, t, 4, :, :].reshape(-1)\n",
    "            month_id = forecast_storage_vol[:, t, 3, :, :].reshape(-1)\n",
    "\n",
    "            # Store extracted values in the output dictionary\n",
    "            output_dict = dict_of_outputs_dicts[feature_key][step]\n",
    "            output_dict.y_score = y_score\n",
    "            output_dict.y_score_prob = y_score_prob\n",
    "            output_dict.y_var = y_var\n",
    "            output_dict.y_var_prob = y_var_prob\n",
    "            output_dict.pg_id = pg_id\n",
    "            output_dict.c_id = c_id\n",
    "            output_dict.step = t + 1\n",
    "            output_dict.month_id = month_id\n",
    "\n",
    "    # Convert the output dictionaries to a DataFrame\n",
    "    df_full = output_to_df(dict_of_outputs_dicts, forecast=True)\n",
    "\n",
    "    # Drop columns related to observed data, as this is forecast-specific\n",
    "    df_full = df_full.drop(columns=['y_true_sb', 'y_true_binary_sb', 'y_true_ns', 'y_true_binary_ns', 'y_true_os', 'y_true_binary_os'])\n",
    "\n",
    "    # and make the vol just for testing and plotting - you should do this for eval as well. \n",
    "    vol_full = np.concatenate((mean_array, mean_class_array, forecast_storage_vol.squeeze().numpy()), axis=1)\n",
    "    vol_full = np.transpose(vol_full, (0, 2, 3, 1))\n",
    "\n",
    "    # this is prolly a more correct way to bo it... \n",
    "    return df_full, vol_full\n",
    "\n",
    "    posterior_dict = {'posterior_list' : posterior_list, 'posterior_list_class': posterior_list_class, 'out_of_sample_vol' : None}\n",
    "    #save_model_outputs(PATH, config, posterior_dict, dict_of_outputs_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_posterior(model, views_vol, df, config, device):\n",
    "#def forecast_posterior(df, config):\n",
    "\n",
    "    \"\"\"\n",
    "    Retrive true forecasts form sample_posterior and generate comprehensive DataFrame and volume representations of these forecasts.\n",
    "\n",
    "    This function handles posterior predictions for multiple features over time, calculates mean and standard deviations,\n",
    "    and compiles these metrics along with metadata into a DataFrame suitable for evaluation or further analysis.\n",
    "    Additionally, it constructs a volume for visualizations or plotting purposes.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model used to generate posterior predictions.\n",
    "        df (pd.DataFrame): DataFrame containing the initial data for generating forecast storage volume (month_id, pg_id, c_id, col, row).\n",
    "        config (dict): Configuration dictionary with model parameters, including 'time_steps' and 'month_range'. BAD TERMONOLOGY!!!!\n",
    "        device (torch.device): Device to run model computations (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, np.ndarray]: \n",
    "            - DataFrame containing processed model outputs with scores, variances, and metadata.\n",
    "            - 4D volume array suitable for testing and plotting.\n",
    "    \"\"\"\n",
    "\n",
    "    # posterior_list, posterior_list_class, _, _, _, _ = sample_posterior(model, views_vol, config, device) <--------------------------\n",
    "\n",
    "    month_range = config['time_steps']\n",
    "\n",
    "    # storage volume for forecasts\n",
    "    forecast_storage_vol = make_forecast_storage_vol(month_range = month_range, to_tensor=True) # CONFIG\n",
    "\n",
    "    # Initialize dictionary to store outputs for different features\n",
    "    dict_of_outputs_dicts = {k: ModelOutputs.make_output_dict(steps = month_range) for k in [\"sb\", \"ns\", \"os\"]} # CONFIG - step is month_range... BAD NAME!\n",
    " \n",
    "    # Calculate mean and standard deviation for posterior predictions and class probabilities\n",
    "    mean_array = np.array(posterior_list).mean(axis=0)\n",
    "    std_array = np.array(posterior_list).std(axis=0)\n",
    "    mean_class_array = np.array(posterior_list_class).mean(axis=0)\n",
    "    std_class_array = np.array(posterior_list_class).std(axis=0)\n",
    "\n",
    "    for t in range(mean_array.shape[0]):  # Iterate over time steps\n",
    "        for i, j in enumerate(dict_of_outputs_dicts.keys()):  # Iterate over feature keys ('sb', 'ns', 'os')\n",
    "            step = f\"step{str(t + 1).zfill(2)}\"\n",
    "\n",
    "            # Extract scores and variances for each feature\n",
    "            y_score = mean_array[t, i, :, :].reshape(-1)\n",
    "            y_score_prob = mean_class_array[t, i, :, :].reshape(-1)\n",
    "            y_var = std_array[t, i, :, :].reshape(-1)\n",
    "            y_var_prob = std_class_array[t, i, :, :].reshape(-1)\n",
    "\n",
    "            # Extract metadata: pg_id, c_id, month_id from the forecast storage volume\n",
    "            pg_id = forecast_storage_vol[:, t, 0, :, :].reshape(-1)\n",
    "            c_id = forecast_storage_vol[:, t, 4, :, :].reshape(-1)\n",
    "            month_id = forecast_storage_vol[:, t, 3, :, :].reshape(-1)\n",
    "\n",
    "            # Store extracted values in the output dictionary\n",
    "            dict_of_outputs_dicts[j][step].y_score = y_score\n",
    "            dict_of_outputs_dicts[j][step].y_score_prob = y_score_prob\n",
    "            dict_of_outputs_dicts[j][step].y_var = y_var\n",
    "            dict_of_outputs_dicts[j][step].y_var_prob = y_var_prob\n",
    "            dict_of_outputs_dicts[j][step].pg_id = pg_id # in theory this should be in the right order\n",
    "            dict_of_outputs_dicts[j][step].c_id = c_id # in theory this should be in the right order\n",
    "            dict_of_outputs_dicts[j][step].step = t +1 # 1 indexed, bc the first step is 1 month ahead\n",
    "            dict_of_outputs_dicts[j][step].month_id = month_id            \n",
    "            \n",
    "#            output_dict = dict_of_outputs_dicts[feature_key][step]\n",
    "#            output_dict.y_score = y_score\n",
    "#            output_dict.y_score_prob = y_score_prob\n",
    "#            output_dict.y_var = y_var\n",
    "#            output_dict.y_var_prob = y_var_prob\n",
    "#            output_dict.pg_id = pg_id\n",
    "#            output_dict.c_id = c_id\n",
    "#            output_dict.step = t + 1\n",
    "#            output_dict.month_id = month_id\n",
    "\n",
    "    # Convert the output dictionaries to a DataFrame\n",
    "    df_full = output_to_df(dict_of_outputs_dicts, forecast=True)\n",
    "\n",
    "    # Drop columns related to observed data, as this is forecast-specific\n",
    "    df_full = df_full.drop(columns=['y_true_sb', 'y_true_binary_sb', 'y_true_ns', 'y_true_binary_ns', 'y_true_os', 'y_true_binary_os'])\n",
    "\n",
    "    # and make the vol just for testing and plotting - you should do this for eval as well. \n",
    "    vol_full = np.concatenate((mean_array, mean_class_array, forecast_storage_vol.squeeze().numpy()), axis=1)\n",
    "    vol_full = np.transpose(vol_full, (0, 2, 3, 1))\n",
    "\n",
    "    # this is prolly a more correct way to bo it... \n",
    "    return df_full, vol_full\n",
    "\n",
    "    posterior_dict = {'posterior_list' : posterior_list, 'posterior_list_class': posterior_list_class, 'out_of_sample_vol' : None}\n",
    "    #save_model_outputs(PATH, config, posterior_dict, dict_of_outputs_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'time_steps': 36, 'sweep': False}\n",
    "print(config['time_steps'])\n",
    "\n",
    "df_full, vol_full = forecast_posterior(None, None, df, config, None)\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vol(vol_full, 36) # so obviously the title is wrong, but otherwise it works!!!! You did it champ <3\n",
    "# but also, make a pretty time laps - really plotting should be a option for both eval and forecast... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old but gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posterior_list, posterior_list_class, out_of_sample_vol, out_of_sample_meta_vol, full_tensor, metadata_tensor = sample_posterior(model, views_vol, config, device)\n",
    "\n",
    "# So need to check that you can sample posterior when running forecasting\n",
    "\n",
    "config = {'time_steps': 36, 'sweep': False}\n",
    "\n",
    "# if eval:\n",
    "#dict_of_eval_dicts = {}\n",
    "#dict_of_eval_dicts = {k: EvaluationMetrics.make_evaluation_dict(steps=config['time_steps']) for k in [\"sb\", \"ns\", \"os\"]}\n",
    "\n",
    "dict_of_outputs_dicts = {}\n",
    "dict_of_outputs_dicts = {k: ModelOutputs.make_output_dict(steps=config['time_steps']) for k in [\"sb\", \"ns\", \"os\"]}\n",
    "\n",
    "# Get mean and std\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "\n",
    "for t in range(mean_array.shape[0]): #  0 of mean array is the temporal dim    \n",
    "#    print(t)\n",
    "\n",
    "#    log_dict = {}\n",
    "#    log_dict[\"monthly/out_sample_month\"] = t +1 # 1 indexed, bc the first step is 1 month ahead\n",
    "\n",
    "    for i, j in enumerate(dict_of_outputs_dicts.keys()): # this is the same as the above but with the dict keys\n",
    "\n",
    "        step = f\"step{str(t+1).zfill(2)}\"\n",
    "\n",
    "        # get the scores\n",
    "        y_score = mean_array[t,i,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[t,i,:,:].reshape(-1) # nu 180x180 \n",
    "\n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[t,i,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[t,i,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        # see this is the out of sample vol - fine for evaluation but not for forecasting\n",
    "        # but also the place where you get the pgm.. \n",
    "\n",
    "        #if eval:\n",
    "        #y_true = out_of_sample_vol[:,t,i,:,:].reshape(-1)  # nu 180x180 . dim 0 is time     THE TRICK IS NOW TO USE A df -> vol and not out_of_sample_vol...\n",
    "        #y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "        # in theorty you could just use the metadata tensor to get pg and c id here\n",
    "        # pg_id = out_of_sample_meta_vol[:,t,0,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 0 is pg_id\n",
    "        # c_id = out_of_sample_meta_vol[:,t,4,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 4 is c_id\n",
    "        # month_id = out_of_sample_meta_vol[:,t,3,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 3 is month_id\n",
    "\n",
    "        # use the synth storage vol to get the pg_id, c_id, and month_id\n",
    "        # the think is that you need to make sure that the metadata tensor is in the right order!!!!!!!!!!!!!!!!!!!!!\n",
    "        # There need to be a test (even is you feel like you have tested the shit out of it in the notebook...)\n",
    "        pg_id = forecast_storage_vol_new[:,t,0,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 0 is pg_id\n",
    "        c_id = forecast_storage_vol_new[:,t,4,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 4 is c_id\n",
    "        month_id = forecast_storage_vol_new[:,t,3,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 3 is month_id\n",
    "\n",
    "        # So, using the metadata tensor, you can get the pg_id, c_id, and month_id for each prediction.\n",
    "        # It is similar to the out_of_sample_meta_vol, but not the specific subset of months\n",
    "        # What you of course need is the extent the metadata tensor 36 months ahead in time on the right dimensions\n",
    "        # But then you good, right?\n",
    "\n",
    "        # if eval\n",
    "#        dict_of_outputs_dicts[j][step].y_true = y_true\n",
    "#        dict_of_outputs_dicts[j][step].y_true_binary = y_true_binary\n",
    "#\n",
    "#        #else: # you need to make sure this works for forecasting\n",
    "#            # in theorty you could just use the metadata tensor to get pg and c id here\n",
    "#        pg_id = out_of_sample_meta_vol[:,t,0,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 0 is pg_id\n",
    "#        c_id = out_of_sample_meta_vol[:,t,4,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 4 is c_id\n",
    "#        month_id = out_of_sample_meta_vol[:,t,3,:,:].reshape(-1)  # nu 180x180, dim 1 is time . dim 2 is feature. feature 3 is month_id\n",
    "#\n",
    "        dict_of_outputs_dicts[j][step].y_score = y_score\n",
    "        dict_of_outputs_dicts[j][step].y_score_prob = y_score_prob\n",
    "        dict_of_outputs_dicts[j][step].y_var = y_var\n",
    "        dict_of_outputs_dicts[j][step].y_var_prob = y_var_prob\n",
    "\n",
    "        dict_of_outputs_dicts[j][step].pg_id = pg_id # in theory this should be in the right order\n",
    "        dict_of_outputs_dicts[j][step].c_id = c_id # in theory this should be in the right order\n",
    "        dict_of_outputs_dicts[j][step].step = t +1 # 1 indexed, bc the first step is 1 month ahead\n",
    "        dict_of_outputs_dicts[j][step].month_id = month_id\n",
    "\n",
    "        # if eval:   \n",
    "        # dict_of_eval_dicts[j][step].MSE = mean_squared_error(y_true, y_score)\n",
    "        # dict_of_eval_dicts[j][step].AP = average_precision_score(y_true_binary, y_score_prob)\n",
    "        # dict_of_eval_dicts[j][step].AUC = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        #dict_of_eval_dicts[j][step].Brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        # note that this actually upates the dict of eval dicts with new stepwise metric values\n",
    "        # log_dict = generate_wandb_log_dict(log_dict, dict_of_eval_dicts, j, step)\n",
    "\n",
    "    # if eval:\n",
    "    # wandb.log(log_dict)\n",
    "\n",
    "# mean_metric_log_dict = generate_wandb_mean_metrics_log_dict(dict_of_eval_dicts)\n",
    "# wandb.log(mean_metric_log_dict)\n",
    "\n",
    "#if not config.sweep:\n",
    "\n",
    "    posterior_dict = {'posterior_list' : posterior_list, 'posterior_list_class': posterior_list_class, 'out_of_sample_vol' : out_of_sample_vol}\n",
    "#    save_model_outputs(PATH, config, posterior_dict, dict_of_outputs_dicts, dict_of_eval_dicts, full_tensor, metadata_tensor)\n",
    "\n",
    "#else:\n",
    "#    print('Running sweep. NO posterior dict, metric dict, or test vol pickled+dumped')\n",
    "\n",
    "\n",
    "df_full = output_to_df(dict_of_outputs_dicts, forecast=True)\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-permute the forecast storage vol\n",
    "forecast_storage_vol_old = forecast_storage_vol_new.permute(0,1,3,4,2).squeeze(dim=0).numpy()\n",
    "print(forecast_storage_vol_old.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot the volume slices\n",
    "plot_vol(forecast_storage_vol_old, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now construct the vol form a df, and we can reconstruct the df from the vol. We can also make a forecasting storage and merge that with (fake) predictions to get a vol similar to the original vol. Now must take that back to a df. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new_vol to df\n",
    "\n",
    "Now, create a new vol and make into a df and ceck that this df can corrospond to the original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start from scracth\n",
    "PATH = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_data.pkl\"\n",
    "df = pd.read_pickle(PATH)\n",
    "\n",
    "vol = df_to_vol(df)\n",
    "\n",
    "vol_fake = generate_fake_vol(vol, month_range=month_range)\n",
    "\n",
    "forecast_storage_vol = make_forecast_storage_vol(df, month_range=month_range)\n",
    "\n",
    "new_vol = merge_vol(forecast_storage_vol, vol_fake)\n",
    "\n",
    "check_vol_equal(vol, new_vol)\n",
    "\n",
    "df_new = vol_to_df(new_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"month_id\"].max())\n",
    "(df_new[\"month_id\"].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_range = 36\n",
    "month_max = df[\"month_id\"].max()\n",
    "month_array = np.arange(month_max - month_range +1, month_max+1)\n",
    "df_sub = df[df['month_id'].isin(month_array)]\n",
    "\n",
    "# only keep the features that are in the new volume\n",
    "df_sub = df_sub[['pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']]\n",
    "\n",
    "df_sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sort both dataframes by pg_id and month_id\n",
    "df_sub = df_sub.sort_values(by=['pg_id', 'month_id'])\n",
    "df_new = df_new.sort_values(by=['pg_id', 'month_id'])\n",
    "\n",
    "# check which columns are not equal \n",
    "for i in range(df_sub.shape[1]):\n",
    "    print(f\"Feature {i}, {df_sub.columns[i]} equal:\", np.array_equal(df_sub.iloc[:, i], df_new.iloc[:, i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good. Now we need to add actual predictions instead of \"fake_predictions\" which were just the last subset of observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# posterior dict to vol to df...\n",
    "Load the correct posterio_dict and the original df (you could prolly use the saved vol, but fuck it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions\n",
    "just some small changes to the functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_to_df_new(vol):\n",
    "    \"\"\"\n",
    "    Converts a 4D volume array back into a DataFrame for spatial-temporal data.\n",
    "    The volume array is expected to have dimensions [n_months, height, width, n_features].\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The input 4D volume array to be converted, with shape \n",
    "                          [n_months, height, width, n_features].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame representation of the volume array containing columns\n",
    "                      'pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best'.\n",
    "                      DataFrame is cleaned to remove rows where 'pg_id' is 0.\n",
    "    \"\"\"\n",
    "    n_months, height, width, n_features = vol.shape\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'pg_id': vol[:, :, :, 0].flatten(),\n",
    "        'col': vol[:, :, :, 1].flatten(),\n",
    "        'row': vol[:, :, :, 2].flatten(),\n",
    "        'month_id': vol[:, :, :, 3].flatten(),\n",
    "        'c_id': vol[:, :, :, 4].flatten(),\n",
    "        'ln_sb_pred': vol[:, :, :, 5].flatten(),\n",
    "        'ln_ns_pred': vol[:, :, :, 6].flatten(),\n",
    "        'ln_os_pred': vol[:, :, :, 7].flatten(),\n",
    "        'proba_os_pred': vol[:, :, :, 8].flatten(),\n",
    "        'proba_ns_pred': vol[:, :, :, 9].flatten(),\n",
    "        'proba_sb_pred': vol[:, :, :, 10].flatten(),\n",
    "    })\n",
    "\n",
    "    # Correct the data types\n",
    "    df['pg_id'] = df['pg_id'].astype(int)\n",
    "    df['col'] = df['col'].astype(int)\n",
    "    df['row'] = df['row'].astype(int)\n",
    "    df['month_id'] = df['month_id'].astype(int)\n",
    "    df['c_id'] = df['c_id'].astype(int)\n",
    "\n",
    "    # Remove rows where 'pg_id' is 0\n",
    "    df = df[df['pg_id'] != 0]\n",
    "\n",
    "    print(f'DataFrame of shape {df.shape} created. Should be (n_months * 180 * 180, 8)')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vol_to_df_oos(vol):\n",
    "    \"\"\"\n",
    "    Converts a 4D volume array back into a DataFrame for spatial-temporal data.\n",
    "    The volume array is expected to have dimensions [n_months, height, width, n_features].\n",
    "\n",
    "    Args:\n",
    "        vol (np.ndarray): The input 4D volume array to be converted, with shape \n",
    "                          [n_months, height, width, n_features].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame representation of the volume array containing columns\n",
    "                      'pg_id', 'col', 'row', 'month_id', 'c_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best'.\n",
    "                      DataFrame is cleaned to remove rows where 'pg_id' is 0.\n",
    "    \"\"\"\n",
    "    n_months, height, width, n_features = vol.shape\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'pg_id': vol[:, :, :, 0].flatten(),\n",
    "        'col': vol[:, :, :, 1].flatten(),\n",
    "        'row': vol[:, :, :, 2].flatten(),\n",
    "        'month_id': vol[:, :, :, 3].flatten(),\n",
    "        'c_id': vol[:, :, :, 4].flatten(),\n",
    "        'ln_sb_best_oos': vol[:, :, :, 5].flatten(),\n",
    "        'ln_ns_best_oos': vol[:, :, :, 6].flatten(),\n",
    "        'ln_os_best_oos': vol[:, :, :, 7].flatten(),\n",
    "    })\n",
    "\n",
    "    # Correct the data types\n",
    "    df['pg_id'] = df['pg_id'].astype(int)\n",
    "    df['col'] = df['col'].astype(int)\n",
    "    df['row'] = df['row'].astype(int)\n",
    "    df['month_id'] = df['month_id'].astype(int)\n",
    "    df['c_id'] = df['c_id'].astype(int)\n",
    "\n",
    "    # Remove rows where 'pg_id' is 0\n",
    "    df = df[df['pg_id'] != 0]\n",
    "\n",
    "    print(f'DataFrame of shape {df.shape} created. Should be (n_months * 180 * 180, 8)')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start from scratch again\n",
    "\n",
    "PATH_df = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_data.pkl\"\n",
    "PATH_posterior_dict = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/generated/posterior_dict_36_calibration_20240613_165106.pkl\"\n",
    "\n",
    "# get the df from the pickle file in raw_data\n",
    "df = pd.read_pickle(PATH_df)\n",
    "\n",
    "# get the posterior_dict from the pickle file in generated\n",
    "with open(PATH_posterior_dict, 'rb') as f:\n",
    "    posterior_dict = pickle.load(f)\n",
    "\n",
    "month_range = 36 # but what even is it in the partion by now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the three lists from the posterior_dict\n",
    "posterior_list, posterior_list_class, out_of_sample_vol = posterior_dict['posterior_list'], posterior_dict['posterior_list_class'], posterior_dict['out_of_sample_vol'] # obviously there will be no out_of_sample_vol with true forecasting...\n",
    "\n",
    "# the posterior_list is a list of list of arrays\n",
    "# the first list is the number of draws from the posterior\n",
    "# the second list is the number of months in the forecast\n",
    "# the arrays are the forecasted volumes for size 3x180x180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe you can just use a modfication of this loop to make the df?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT!!!\n",
    "\n",
    "So, see if you can make a df from this - remember two things: \n",
    "- you don't have an out_of_sample_vol for true forecasts. \n",
    "- And out_of_sample_vol also don't have any features beyond history of violence..  \n",
    "\n",
    "So this is to a large extent a validation execice we need to see if we can:\n",
    "- turn it back into a df\n",
    "- get the same results as with the get_log_dict\n",
    "- Anf then figure out how to get the same resultes with the df of vol... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol):\n",
    "\n",
    "    \"\"\"Return a dictionary of metrics for the monthly out-of-sample predictions for W&B.\"\"\"\n",
    "\n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = i\n",
    "\n",
    "\n",
    "    #Fix in a sec when you see if it runs at all.... \n",
    "    for j in range(3): #(config.targets): # TARGETS IS & BUT THIS SHOULD BE 3!!!!!\n",
    "\n",
    "        y_score = mean_array[i,j,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[i,j,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[i,j,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[i,j,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        y_true = out_of_sample_vol[:,i,j,:,:].reshape(-1)  # nu 180x180 . dim 0 is time\n",
    "        y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        log_dict[f\"monthly/mean_squared_error{j}\"] = mse\n",
    "        log_dict[f\"monthly/average_precision_score{j}\"] = ap\n",
    "        log_dict[f\"monthly/roc_auc_score{j}\"] = auc\n",
    "        log_dict[f\"monthly/brier_score_loss{j}\"] = brier\n",
    "\n",
    "\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKAY! lets start with the thing from eval\n",
    "# Get mean and std\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "out_sample_month_list = [] # only used for pickle...\n",
    "ap_list = []\n",
    "mse_list = []\n",
    "auc_list = []\n",
    "brier_list = []\n",
    "\n",
    "#NEW\n",
    "log_dict_list = []\n",
    "\n",
    "for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim\n",
    "\n",
    "    #y_score = mean_array[i].reshape(-1) # make it 1d  # nu 180x180\n",
    "    #y_score_prob = mean_class_array[i].reshape(-1) # nu 180x180\n",
    "\n",
    "    # do not really know what to do with these yet.\n",
    "#    y_var = std_array[i].reshape(-1)  # nu 180x180\n",
    "#    y_var_prob = std_class_array[i].reshape(-1)  # nu 180x180\n",
    "\n",
    "#    y_true = out_of_sample_vol[:,i].reshape(-1)  # nu 180x180 . dim 0 is time\n",
    " #   y_true_binary = (y_true > 0) * 1\n",
    "#\n",
    "    # log the metrics to WandB - but why here? \n",
    "    log_dict = get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol)# so at least it gets reported sep.\n",
    "    log_dict_list.append(log_dict)\n",
    "\n",
    "    #wandb.log(log_dict)\n",
    "\n",
    "    # YOU KNOW THIS IS WRONG ALREADY!!!!\n",
    "\n",
    "#    # this could be a function in utils_wandb or in common_utils... \n",
    "#    mse = mean_squared_error(y_true, y_score)  \n",
    "#    ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "#    auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "#    brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "#\n",
    "#    out_sample_month_list.append(i) # only used for pickle...\n",
    "#    mse_list.append(mse)\n",
    "#    ap_list.append(ap) # add to list.\n",
    "#    auc_list.append(auc)\n",
    "#    brier_list.append(brier)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(log_dict_list, num_months, feature = 0):\n",
    "    \"\"\"\n",
    "    Plots MSE, Average Precision, ROC AUC, and Brier Score for each month from log_dict_list.\n",
    "\n",
    "    Args:\n",
    "        log_dict_list (list of dict): List of dictionaries with monthly metrics.\n",
    "        num_months (int): Number of months to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics for each month\n",
    "    mse_list = []\n",
    "    ap_list = []\n",
    "    auc_list = []\n",
    "    brier_list = []\n",
    "\n",
    "    # Iterate over the log_dict_list and extract the metrics\n",
    "    for i in range(num_months):\n",
    "        mse_list.append(log_dict_list[i][f'monthly/mean_squared_error{feature}'])\n",
    "        ap_list.append(log_dict_list[i][f'monthly/average_precision_score{feature}'])\n",
    "        auc_list.append(log_dict_list[i][f'monthly/roc_auc_score{feature}'])\n",
    "        brier_list.append(log_dict_list[i][f'monthly/brier_score_loss{feature}'])\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "    # Plot MSE\n",
    "    axs[0, 0].plot(range(1, len(mse_list) + 1), mse_list, marker='o', color='b', label='MSE')\n",
    "    axs[0, 0].set_title('Mean Squared Error')\n",
    "    axs[0, 0].set_xlabel('Month')\n",
    "    axs[0, 0].set_ylabel('MSE')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot Average Precision\n",
    "    axs[0, 1].plot(range(1, len(ap_list) + 1), ap_list, marker='o', color='g', label='Average Precision')\n",
    "    axs[0, 1].set_title('Average Precision Score')\n",
    "    axs[0, 1].set_xlabel('Month')\n",
    "    axs[0, 1].set_ylabel('AP Score')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # Plot ROC AUC\n",
    "    axs[1, 0].plot(range(1, len(auc_list) + 1), auc_list, marker='o', color='r', label='ROC AUC')\n",
    "    axs[1, 0].set_title('ROC AUC Score')\n",
    "    axs[1, 0].set_xlabel('Month')\n",
    "    axs[1, 0].set_ylabel('AUC Score')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Plot Brier Score\n",
    "    axs[1, 1].plot(range(1, len(brier_list) + 1), brier_list, marker='o', color='m', label='Brier Score')\n",
    "    axs[1, 1].set_title('Brier Score Loss')\n",
    "    axs[1, 1].set_xlabel('Month')\n",
    "    axs[1, 1].set_ylabel('Brier Score')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    # add a title\n",
    "    plt.suptitle(f'Metrics for Feature {feature} Over {num_months} Months', fontsize=16)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(log_dict_list, 36, 0)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(log_dict_list, 36, 1)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(log_dict_list, 36, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol):\n",
    "\n",
    "    \"\"\"Return a dictionary of metrics for the monthly out-of-sample predictions for W&B.\"\"\"\n",
    "\n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = i\n",
    "\n",
    "\n",
    "    #Fix in a sec when you see if it runs at all.... \n",
    "    for j in range(3): #(config.targets): # TARGETS IS & BUT THIS SHOULD BE 3!!!!!\n",
    "\n",
    "        y_score = mean_array[i,j,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[i,j,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[i,j,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[i,j,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        y_true = out_of_sample_vol[:,i,j,:,:].reshape(-1)  # nu 180x180 . dim 0 is time\n",
    "        y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        log_dict[f\"monthly/mean_squared_error{j}\"] = mse\n",
    "        log_dict[f\"monthly/average_precision_score{j}\"] = ap\n",
    "        log_dict[f\"monthly/roc_auc_score{j}\"] = auc\n",
    "        log_dict[f\"monthly/brier_score_loss{j}\"] = brier\n",
    "\n",
    "\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "# SHIT THAT WORKS!!!\n",
    "## start reading here!!! :\n",
    "\n",
    "This is the thing that need to be the fundament.\n",
    "First I need to see that this works with the storage_array or another way of retaining or retreaving pg_id and month_id\n",
    "One possible way to do this is by using the meta_tensor you have started on below\n",
    "An alternative would be to to make a full full_tensor. I.e one that contains both the 3 prime feature and the \"meta\" fatures\n",
    "Ones we have proven we can get month_id and pgm back we need to see a out-of-sampel solution where we go beyond what is in the df\n",
    "And then I think we need to go back streamline the evaluation process so it follows. WHile doing so I must\n",
    "    \n",
    "    - keep true forecasting firmly in the mind\n",
    "    - Think about what, if anything, can be abstracted out to common_utils for the sake of getting it right for the stepshifters - maybe wandb stuff\n",
    "    - And while ad it, make sure you use you new data class to store the monthly metrics\n",
    "    - The more you can absract out and make general, the simple it should be\n",
    "    - And while you add it, make sure to check if the partitions are the same as in paper. If they are, get the results down and finish paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets start from scratch again\n",
    "\n",
    "PATH_df = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/raw/calibration_viewser_df.pkl\"\n",
    "PATH_posterior_dict = \"/home/simon/Documents/scripts/views_pipeline/models/purple_alien/data/generated/posterior_dict_36_calibration_20240613_165106.pkl\"\n",
    "\n",
    "# get the df from the pickle file in raw_data\n",
    "df = pd.read_pickle(PATH_df)\n",
    "\n",
    "# get the posterior_dict from the pickle file in generated\n",
    "with open(PATH_posterior_dict, 'rb') as f:\n",
    "    posterior_dict = pickle.load(f)\n",
    "\n",
    "month_range = 36\n",
    "\n",
    "# get the three lists from the posterior_dict - we make the out_of_sample_vol later\n",
    "posterior_list, posterior_list_class, _ = posterior_dict['posterior_list'], posterior_dict['posterior_list_class'], posterior_dict['out_of_sample_vol'] # obviously there will be no out_of_sample_vol with true forecasting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_full_tensor(views_vol): #, config, device):\n",
    "\n",
    "    \"\"\"Uses to get the features for the full tensor\n",
    "    Used for out-of-sample predictions for both evaluation and forecasting, depending on the run_type (partition). \n",
    "    The test tensor is of size 1 x config.time_steps x config.input_channels x 180 x 180.\"\"\"\n",
    "\n",
    "    ln_best_sb_idx = 5#config.first_feature_idx # 5 = ln_best_sb\n",
    "    last_feature_idx = ln_best_sb_idx + month_range #config.input_channels\n",
    "\n",
    "    print(f'views_vol shape {views_vol.shape}')\n",
    "\n",
    "    # THIS IS WHERE YOU LOOSE THE OTHE FEATURES!!!!\n",
    "    full_tensor = torch.tensor(views_vol).float().unsqueeze(dim=0).permute(0,1,4,2,3)[:, :, ln_best_sb_idx:last_feature_idx, :, :] \n",
    "\n",
    "    # Make a metadata tensor with evrything else\n",
    "    metadata_tensor = torch.tensor(views_vol).float().unsqueeze(dim=0).permute(0,1,4,2,3)[:, :, :ln_best_sb_idx, :, :]\n",
    "\n",
    "    print(f'full_tensor shape {full_tensor.shape}')\n",
    "\n",
    "    return full_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_vol = df_to_vol(df)\n",
    "views_vol = views_vol.copy() # why the fuck face this works I swear I have no idea\n",
    "\n",
    "print(views_vol.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tensor = get_full_tensor(views_vol ) #, config, device) # better cal this evel tensor\n",
    "\n",
    "#out_of_sample_vol = full_tensor[:,-config.time_steps:,:,:,:].cpu().numpy() # From the test tensor get the out-of-sample time_steps. \n",
    "out_of_sample_vol = full_tensor[:,-month_range:,:,:,:] #.cpu().numpy() # From the test tensor get the out-of-sample time_steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKAY! lets start with the thing from eval\n",
    "# Get mean and std\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "out_sample_month_list = [] # only used for pickle...\n",
    "ap_list = []\n",
    "mse_list = []\n",
    "auc_list = []\n",
    "brier_list = []\n",
    "\n",
    "#NEW\n",
    "log_dict_list = []\n",
    "feature_dict_list = []\n",
    "\n",
    "for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim\n",
    "\n",
    "    # log the metrics to WandB - but why here? \n",
    "    #log_dict = get_log_dict(i, mean_array, mean_class_array, std_array, std_class_array, out_of_sample_vol)# so at least it gets reported sep.\n",
    "    \n",
    "    \n",
    "    log_dict = {}\n",
    "    log_dict[\"monthly/out_sample_month\"] = i\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "\n",
    "    #Fix in a sec when you see if it runs at all.... \n",
    "    for j in range(3): #(config.targets): # TARGETS IS & BUT THIS SHOULD BE 3!!!!!\n",
    "\n",
    "        y_score = mean_array[i,j,:,:].reshape(-1) # make it 1d  # nu 180x180 \n",
    "        y_score_prob = mean_class_array[i,j,:,:].reshape(-1) # nu 180x180 \n",
    "        \n",
    "        # do not really know what to do with these yet.\n",
    "        y_var = std_array[i,j,:,:].reshape(-1)  # nu 180x180  \n",
    "        y_var_prob = std_class_array[i,j,:,:].reshape(-1)  # nu 180x180 \n",
    "\n",
    "        y_true = out_of_sample_vol[:,i,j,:,:].reshape(-1)  # nu 180x180 . dim 0 is time     THE TRICK IS NOW TO USE A df -> vol and not out_of_sample_vol...\n",
    "        y_true_binary = (y_true > 0) * 1\n",
    "\n",
    "        # data\n",
    "        feature_dict[f\"y_score{j}\"] = y_score\n",
    "        feature_dict[f\"y_score_prob{j}\"] = y_score_prob\n",
    "        feature_dict[f\"y_var{j}\"] = y_var\n",
    "        feature_dict[f\"y_var_prob{j}\"] = y_var_prob\n",
    "        feature_dict[f\"y_true{j}\"] = y_true\n",
    "        feature_dict[f\"y_true_binary{j}\"] = y_true_binary\n",
    "\n",
    "        # metrics\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        log_dict[f\"monthly/mean_squared_error{j}\"] = mse\n",
    "        log_dict[f\"monthly/average_precision_score{j}\"] = ap\n",
    "        log_dict[f\"monthly/roc_auc_score{j}\"] = auc\n",
    "        log_dict[f\"monthly/brier_score_loss{j}\"] = brier\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_dict_list.append(feature_dict)\n",
    "    log_dict_list.append(log_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = pd.DataFrame.from_dict(feature_dict_list).apply(pd.Series.explode)\n",
    "\n",
    "# reset the index and keep the old index as a column month\n",
    "df_test2 = df_test2.reset_index().rename(columns = {\"index\": \"month\"})\n",
    "\n",
    "df_test2 # we don't konw if the month order is correct yet... But I highly suspect that it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all datatyps in columns\n",
    "df_test2.dtypes\n",
    "\n",
    "# change all columns to float\n",
    "df_test2 = df_test2.astype(float)\n",
    "\n",
    "# check all datatyps in columns\n",
    "df_test2.dtypes\n",
    "\n",
    "# make the binary columns integers\n",
    "df_test2 = df_test2.astype({\"y_true_binary0\": int, \"y_true_binary1\": int, \"y_true_binary2\": int})\n",
    "\n",
    "# check all datatyps in columns\n",
    "df_test2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_precision_score(df_test2[\"y_true_binary0\"], df_test2[\"y_score_prob0\"]))\n",
    "print(average_precision_score(df_test2[\"y_true_binary1\"], df_test2[\"y_score_prob1\"]))\n",
    "print(average_precision_score(df_test2[\"y_true_binary2\"], df_test2[\"y_score_prob2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay so this works... lest recreat the monthly metrics for all features and the plot from above\n",
    "\n",
    "def plot_metrics(df_test2, feature = 0):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots MSE, Average Precision, ROC AUC, and Brier Score for each month from log_dict_list.\n",
    "\n",
    "    Args:\n",
    "        log_dict_list (list of dict): List of dictionaries with monthly metrics.\n",
    "        num_months (int): Number of months to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store metrics for each month\n",
    "    mse_list = []\n",
    "    ap_list = []\n",
    "    auc_list = []\n",
    "    brier_list = []\n",
    "\n",
    "\n",
    "    # Iterate over the log_dict_list and extract the metrics\n",
    "    for i in df_test2[\"month\"].unique():\n",
    "\n",
    "        y_score = df_test2[df_test2[\"month\"] == i][f\"y_score{feature}\"]\n",
    "        y_score_prob = df_test2[df_test2[\"month\"] == i][f\"y_score_prob{feature}\"]\n",
    "        y_true = df_test2[df_test2[\"month\"] == i][f\"y_true{feature}\"]\n",
    "        y_true_binary = df_test2[df_test2[\"month\"] == i][f\"y_true_binary{feature}\"]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        ap_list.append(ap)\n",
    "        auc_list.append(auc)\n",
    "        brier_list.append(brier)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "\n",
    "    # Plot MSE\n",
    "    axs[0, 0].plot(range(1, len(mse_list) + 1), mse_list, marker='o', color='b', label='MSE')\n",
    "    axs[0, 0].set_title('Mean Squared Error')\n",
    "    axs[0, 0].set_xlabel('Month')\n",
    "    axs[0, 0].set_ylabel('MSE')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot Average Precision\n",
    "    axs[0, 1].plot(range(1, len(ap_list) + 1), ap_list, marker='o', color='g', label='Average Precision')\n",
    "    axs[0, 1].set_title('Average Precision Score')\n",
    "    axs[0, 1].set_xlabel('Month')\n",
    "    axs[0, 1].set_ylabel('AP Score')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # Plot ROC AUC\n",
    "    axs[1, 0].plot(range(1, len(auc_list) + 1), auc_list, marker='o', color='r', label='ROC AUC')\n",
    "    axs[1, 0].set_title('ROC AUC Score')\n",
    "    axs[1, 0].set_xlabel('Month')\n",
    "    axs[1, 0].set_ylabel('AUC Score')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Plot Brier Score\n",
    "    axs[1, 1].plot(range(1, len(brier_list) + 1), brier_list, marker='o', color='m', label='Brier Score')\n",
    "    axs[1, 1].set_title('Brier Score Loss')\n",
    "    axs[1, 1].set_xlabel('Month')\n",
    "    axs[1, 1].set_ylabel('Brier Score')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    # add a title\n",
    "    plt.suptitle(f'Metrics for Feature {feature} Over {df_test2[\"month\"].max()} Months', fontsize=16)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plots\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 0)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 1)\n",
    "\n",
    "# Example usage:\n",
    "plot_metrics(df_test2, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.from_dict(log_dict_list)\n",
    "df_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shit that works is right above\n",
    "\n",
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trynna make the df this style\n",
    "mean_array = np.array(posterior_list).mean(axis = 0) # get mean for each month!\n",
    "#std_array = np.array(posterior_list).std(axis = 0)\n",
    "\n",
    "mean_class_array = np.array(posterior_list_class).mean(axis = 0) # get mean for each month!\n",
    "#std_class_array = np.array(posterior_list_class).std(axis = 0)\n",
    "\n",
    "\n",
    "#dict_features = {}\n",
    "\n",
    "df_features = pd.DataFrame()\n",
    "\n",
    "for j in range(3):\n",
    "#for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim. you could use an actual month_id array here\n",
    "\n",
    "#    print(i)\n",
    "\n",
    "#    dict_features[j] = {}\n",
    "\n",
    "    y_score_list = []\n",
    "    y_score_prob_list = []\n",
    "    y_true_list = []\n",
    "    y_true_binary_list = []\n",
    "    month_list  = []\n",
    "\n",
    "    #for j in range(3): # 3 features\n",
    "    for i in range(mean_array.shape[0]): #  0 of mean array is the temporal dim\n",
    "\n",
    "        y_score_list.append(mean_array[i,j,:,:].reshape(-1)) # make it 1d  # nu 180x180\n",
    "        y_score_prob_list.append(mean_class_array[i,j,:,:].reshape(-1)) # nu 180x180\n",
    "        y_true_list.append(out_of_sample_vol[:,i,j,:,:].reshape(-1))  # nu 180x180 . dim 0 is time\n",
    "        y_true_binary_list.append((y_true > 0) * 1)\n",
    "\n",
    "        # month of same size as y_score_list\n",
    "\n",
    "        if j == 0: # only do this once\n",
    "            month_list.append(np.array([i]*len(y_score_list[i])))\n",
    "\n",
    "    # lets go straight to df\n",
    "    if j == 0:\n",
    "        df_features[f'month'] = np.concatenate(month_list)\n",
    "    \n",
    "    df_features[f'y_score_{j}'] = np.concatenate(y_score_list)\n",
    "    df_features[f'y_score_prob_{j}'] = np.concatenate(y_score_prob_list)\n",
    "    df_features[f'y_true_{j}'] = np.concatenate(y_true_list)\n",
    "    df_features[f'y_true_binary_{j}'] = np.concatenate(y_true_binary_list)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#    dict_features[j][\"y_score\"] = y_score_list\n",
    "#   dict_features[j][\"y_score_prob\"] = y_score_prob_list\n",
    "#    dict_features[j][\"y_true\"] = y_true_list\n",
    "#    dict_features[j][\"y_true_binary\"] = y_true_binary_list\n",
    "#    dict_features[j][\"month\"] = month_list\n",
    "\n",
    "    # and now month so just i in and array the right size\n",
    "    #dict_features[j]['month'] = np.array([i]*len(y_score_list[0])) # just make it the right size\n",
    "\n",
    "\n",
    "# now the dict to df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(df_features[df_features['month'] == 0]['y_true_binary_1'], df_features[df_features['month'] == 0]['y_score_prob_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firts feature 0,1, or 2. them the output type - y_score, y_score_prob, y_true, y_true_binary. then the month and then the 180x180 array\n",
    "\n",
    "month_array = np.array(dict_features[0][\"month\"]).reshape(-1)\n",
    "y_score_array_0 = np.array(dict_features[0][\"y_score\"]).reshape(-1)\n",
    "y_score_prob_array_0 = np.array(dict_features[0][\"y_score_prob\"]).reshape(-1)\n",
    "y_true_array_0 = np.array(dict_features[0][\"y_true\"]).reshape(-1)\n",
    "y_true_binary_array_0 = np.array(dict_features[0][\"y_true_binary\"]).reshape(-1)\n",
    "\n",
    "y_score_array_1 = np.array(dict_features[1][\"y_score\"]).reshape(-1)\n",
    "y_score_prob_array_1 = np.array(dict_features[1][\"y_score_prob\"]).reshape(-1)\n",
    "y_true_array_1 = np.array(dict_features[1][\"y_true\"]).reshape(-1)\n",
    "y_true_binary_array_1 = np.array(dict_features[1][\"y_true_binary\"]).reshape(-1)\n",
    "\n",
    "y_score_array_2 = np.array(dict_features[2][\"y_score\"]).reshape(-1)\n",
    "y_score_prob_array_2 = np.array(dict_features[2][\"y_score_prob\"]).reshape(-1)\n",
    "y_true_array_2 = np.array(dict_features[2][\"y_true\"]).reshape(-1)\n",
    "y_true_binary_array_2 = np.array(dict_features[2][\"y_true_binary\"]).reshape(-1)\n",
    "\n",
    "df_features = pd.DataFrame({\n",
    "    'month_id': month_array,\n",
    "    'y_score_0': y_score_array_0,\n",
    "    'y_score_prob_0': y_score_prob_array_0,\n",
    "    'y_true_0': y_true_array_0,\n",
    "    'y_true_binary_0': y_true_binary_array_0,\n",
    "    'y_score_1': y_score_array_1,\n",
    "    'y_score_prob_1': y_score_prob_array_1,\n",
    "    'y_true_1': y_true_array_1,\n",
    "    'y_true_binary_1': y_true_binary_array_1,\n",
    "    'y_score_2': y_score_array_2,\n",
    "    'y_score_prob_2': y_score_prob_array_2,\n",
    "    'y_true_2': y_true_array_2,\n",
    "    'y_true_binary_2': y_true_binary_array_2,\n",
    "})\n",
    "\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(df_features[\"y_true_binary_0\"], df_features[\"y_score_prob_0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the monthly metrics for each feature in the df\n",
    "\n",
    "for i in df_features[\"month_id\"].unique():\n",
    "    print(i)\n",
    "    df_month = df_features[df_features[\"month_id\"] == i]\n",
    "\n",
    "    for j in range(3):\n",
    "        y_score = df_month[f\"y_score_{j}\"]\n",
    "        y_score_prob = df_month[f\"y_score_prob_{j}\"]\n",
    "        y_true = df_month[f\"y_true_{j}\"]\n",
    "        y_true_binary = df_month[f\"y_true_binary_{j}\"]\n",
    "\n",
    "        mse = mean_squared_error(y_true, y_score)\n",
    "        ap = average_precision_score(y_true_binary, y_score_prob)\n",
    "        auc = roc_auc_score(y_true_binary, y_score_prob)\n",
    "        brier = brier_score_loss(y_true_binary, y_score_prob)\n",
    "\n",
    "        print(f\"Feature {j} - MSE: {mse}, AP: {ap}, AUC: {auc}, Brier: {brier}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the posterior_list to a numpy array of shape (n_draws, n_months, 3, 180, 180)\n",
    "posterior_array = np.array(posterior_list)\n",
    "print(posterior_array.shape)\n",
    "\n",
    "# revesr the order of the months\n",
    "posterior_array = np.flip(posterior_array, axis = 1)\n",
    "\n",
    "# for now we can take the mean (later hdi) of the posterior_array as a point estimate\n",
    "posterior_mean = np.mean(posterior_array, axis=0)\n",
    "print(posterior_mean.shape)\n",
    "\n",
    "# reverse the order of the months\n",
    "#posterior_mean = np.flip(posterior_mean, axis = 0)\n",
    "\n",
    "# reshape the posterior_mean to be similar to the vol\n",
    "posterior_mean = np.transpose(posterior_mean, (0, 2, 3, 1))\n",
    "print(posterior_mean.shape)\n",
    "\n",
    "# reverse the order of the months\n",
    "#posterior_mean = np.flip(posterior_mean, axis = 0)\n",
    "\n",
    "# and for class\n",
    "posterior_class_array = np.array(posterior_list_class)\n",
    "print(posterior_class_array.shape)\n",
    "\n",
    "# revesr the order of the months\n",
    "#posterior_class_array = np.flip(posterior_class_array, axis = 1)\n",
    "\n",
    "# for now we can take the mean (later hdi) of the posterior_array as a point estimate\n",
    "posterior_class_mean = np.mean(posterior_class_array, axis=0)\n",
    "print(posterior_class_mean.shape)\n",
    "\n",
    "# reverse the order of the months   \n",
    "#posterior_class_mean = np.flip(posterior_class_mean, axis = 0)\n",
    "\n",
    "# reshape the posterior_mean to be similar to the vol\n",
    "posterior_class_mean = np.transpose(posterior_class_mean, (0, 2, 3, 1))\n",
    "print(posterior_class_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the posterior_mean and the posterior_class_mean\n",
    "posterior_all_mean = np.concatenate([posterior_mean, posterior_class_mean], axis=-1)\n",
    "\n",
    "# reverse the order of the months\n",
    "#posterior_all_mean = np.flip(posterior_all_mean, axis = 0)\n",
    "\n",
    "print(posterior_all_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_storage_vol = make_forecast_storage_vol(df, month_range=month_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vol = merge_vol(forecast_storage_vol, posterior_all_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we merge with the out_of_sample_vol instead?\n",
    "\n",
    "# remove batch dimension from out_of_sample_vol\n",
    "out_of_sample_vol_nb = np.squeeze(out_of_sample_vol, axis=0)\n",
    "\n",
    "# reshape the out_of_sample_vol_nb to be similar to the vol\n",
    "out_of_sample_vol_nb = np.transpose(out_of_sample_vol_nb, (0, 2, 3, 1))\n",
    "\n",
    "#merge the out_of_sample_vol_nb with the forecast_storage_vol\n",
    "new_vol_oos = merge_vol(forecast_storage_vol, out_of_sample_vol_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_vol(new_vol, 3)\n",
    "plot_vol(new_vol_oos, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = vol_to_df_new(new_vol)\n",
    "df_new_oos = vol_to_df_oos(new_vol_oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['month_id'].max()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so the correctly the df_new have all months_ids moved forward. But since the evaluation set is here part of the original df, I'll just push it back to the original month_ids\n",
    "df_new[\"month_id\"] = df_new[\"month_id\"] - month_range\n",
    "print(df_new[\"month_id\"].max())\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now subset the df to only include the last month_range months\n",
    "month_max = df[\"month_id\"].max()\n",
    "month_array = np.arange(month_max - month_range +1, month_max+1)\n",
    "df_sub = df[df['month_id'].isin(month_array)]\n",
    "\n",
    "print(df_sub['month_id'].max())\n",
    "\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how does that compare the df_new_oos\n",
    "\n",
    "# you need to take the month_id back to the oriringal month_id\n",
    "df_new_oos[\"month_id\"] = df_new_oos[\"month_id\"] - month_range\n",
    "df_new_oos[\"month_id\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df (df_merged) which is df_new but with ln_sb_best, ln_ns_best, ln_os_best from df_sub merged by pg_id and month_id\n",
    "\n",
    "# real\n",
    "#df_merged = df_new.merge(df_sub[['pg_id', 'month_id', 'ln_sb_best', 'ln_ns_best', 'ln_os_best']], on=['pg_id', 'month_id'], how='left')\n",
    "\n",
    "# oos\n",
    "df_merged = df_new.merge(df_new_oos[['pg_id', 'month_id', 'ln_sb_best_oos', 'ln_ns_best_oos', 'ln_os_best_oos']], on=['pg_id', 'month_id'], how='left')\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now a correlation plot between the ln_sb_best, ln_ns_best, ln_os_best and ln_sb_pred, ln_ns_pred, ln_os_pred and the same for the probas\n",
    "#df_merged[['ln_sb_best', 'ln_ns_best', 'ln_os_best', 'ln_sb_pred', 'ln_ns_pred', 'ln_os_pred', 'proba_sb_pred', 'proba_ns_pred', 'proba_os_pred']].corr()\n",
    "\n",
    "df_merged[['ln_sb_best_oos', 'ln_ns_best_oos', 'ln_os_best_oos', 'ln_sb_pred', 'ln_ns_pred', 'ln_os_pred', 'proba_sb_pred', 'proba_ns_pred', 'proba_os_pred']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now a correlation plot between the ln_sb_best, ln_ns_best, ln_os_best and ln_sb_pred, ln_ns_pred, ln_os_pred and the same for the probas\n",
    "df_merged[['ln_sb_best', 'ln_ns_best', 'ln_os_best', 'ln_sb_pred', 'ln_ns_pred', 'ln_os_pred', 'proba_sb_pred', 'proba_ns_pred', 'proba_os_pred']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# binarize the ln's\n",
    "# class_sb_best = df_merged['ln_sb_best'] > 0\n",
    "# class_ns_best = df_merged['ln_ns_best'] > 0\n",
    "# class_os_best = df_merged['ln_os_best'] > 0\n",
    "\n",
    "\n",
    "# binarize the ln's\n",
    "class_sb_best = df_merged['ln_sb_best_oos'] > 0\n",
    "class_ns_best = df_merged['ln_ns_best_oos'] > 0\n",
    "class_os_best = df_merged['ln_os_best_oos'] > 0\n",
    "\n",
    "# get the auroc for the probas\n",
    "auroc_sb = roc_auc_score(class_sb_best, df_merged['proba_sb_pred'])\n",
    "auroc_ns = roc_auc_score(class_ns_best, df_merged['proba_ns_pred'])\n",
    "auroc_os = roc_auc_score(class_os_best, df_merged['proba_os_pred'])\n",
    "\n",
    "# get the average precision for the probas\n",
    "ap_sb = average_precision_score(class_sb_best, df_merged['proba_sb_pred'])\n",
    "ap_ns = average_precision_score(class_ns_best, df_merged['proba_ns_pred'])\n",
    "ap_os = average_precision_score(class_os_best, df_merged['proba_os_pred'])\n",
    "\n",
    "# get the brier score for the probas\n",
    "brier_sb = brier_score_loss(class_sb_best, df_merged['proba_sb_pred'])\n",
    "brier_ns = brier_score_loss(class_ns_best, df_merged['proba_ns_pred'])\n",
    "brier_os = brier_score_loss(class_os_best, df_merged['proba_os_pred'])\n",
    "\n",
    "# get the mse for the ln's\n",
    "mse_sb = mean_squared_error(df_merged['ln_sb_best_oos'], df_merged['ln_sb_pred'])\n",
    "mse_ns = mean_squared_error(df_merged['ln_ns_best_oos'], df_merged['ln_ns_pred'])\n",
    "mse_os = mean_squared_error(df_merged['ln_os_best_oos'], df_merged['ln_os_pred'])\n",
    "\n",
    "# create a nice table with all the results\n",
    "results = pd.DataFrame({\n",
    "    'auroc': [auroc_sb, auroc_ns, auroc_os],\n",
    "    'ap': [ap_sb, ap_ns, ap_os],\n",
    "    'brier': [brier_sb, brier_ns, brier_os],\n",
    "    'mse': [mse_sb, mse_ns, mse_os]\n",
    "}, index=['sb', 'ns', 'os'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you are not even sure if the querysets are the same... Should be but are you 100%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how that corropsonds to is we use the out_of_sample_vol\n",
    "\n",
    "# remove the batch dimension\n",
    "out_of_sample_vol_no_batch = out_of_sample_vol[0]\n",
    "\n",
    "# reshape the out_of_sample_vol_no_batch to be similar to the vol\n",
    "out_of_sample_vol_no_batch = np.transpose(out_of_sample_vol_no_batch, (0, 2, 3, 1))\n",
    "print(out_of_sample_vol_no_batch.shape)\n",
    "\n",
    "# now compare this to the 48 month of the vol\n",
    "vol_3_36 = vol[-month_range:, :, :, 5:]\n",
    "print(vol_3_36.shape)\n",
    "\n",
    "# compare the two\n",
    "print(np.array_equal(out_of_sample_vol_no_batch, vol_3_36))\n",
    "\n",
    "# but are they near?\n",
    "print(np.allclose(out_of_sample_vol_no_batch, vol_3_36))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but is there any correlation between the out_of_sample_vol and the vol_3_48?\n",
    "\n",
    "# get the corr between the two arrays\n",
    "print(np.corrcoef(out_of_sample_vol_no_batch.flatten(), vol_3_36.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is we forst sum each to see the total sum of the arrays and then substrat one from the other to see what the difference is\n",
    "print(np.sum(out_of_sample_vol_no_batch))\n",
    "print(np.sum(vol_3_36))\n",
    "\n",
    "print(np.sum(out_of_sample_vol_no_batch) - np.sum(vol_3_36)) # Just a rounding error it seems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot placehodlers for the two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now month specific metrics..\n",
    "\n",
    "# first create the list to store the individaul monthly metrics\n",
    "\n",
    "#auroc\n",
    "auroc_sb_list = []\n",
    "auroc_ns_list = []\n",
    "auroc_os_list = []\n",
    "\n",
    "#ap\n",
    "ap_sb_list = []\n",
    "ap_ns_list = []\n",
    "ap_os_list = []\n",
    "\n",
    "#brier\n",
    "brier_sb_list = []\n",
    "brier_ns_list = []\n",
    "brier_os_list = []\n",
    "\n",
    "#mse\n",
    "mse_sb_list = []\n",
    "mse_ns_list = []\n",
    "mse_os_list = []\n",
    "\n",
    "month_array = df_merged['month_id'].unique()\n",
    "#print(month_array)\n",
    "\n",
    "# loop through the months\n",
    "for i in month_array:\n",
    "    #print(i)\n",
    "\n",
    "    # subset the df_merged to only include the month\n",
    "    df_sub = df_merged[df_merged['month_id'] == i]\n",
    "\n",
    "    # binarize the ln's for auroc, ap and brier\n",
    "    #class_sb_best = df_sub['ln_sb_best'] > 0\n",
    "    #class_ns_best = df_sub['ln_ns_best'] > 0\n",
    "    #class_os_best = df_sub['ln_os_best'] > 0\n",
    "\n",
    "    # binarize the ln's for auroc, ap and brier\n",
    "    class_sb_best = df_sub['ln_sb_best_oos'] > 0\n",
    "    class_ns_best = df_sub['ln_ns_best_oos'] > 0\n",
    "    class_os_best = df_sub['ln_os_best_oos'] > 0\n",
    "\n",
    "    # get the auroc for the probas and append to the list\n",
    "    auroc_sb_list.append(roc_auc_score(class_sb_best, df_sub['proba_sb_pred']))\n",
    "    auroc_ns_list.append(roc_auc_score(class_ns_best, df_sub['proba_ns_pred']))\n",
    "    auroc_os_list.append(roc_auc_score(class_os_best, df_sub['proba_os_pred']))\n",
    "\n",
    "    # get the average precision for the probas and append to the list\n",
    "    ap_sb_list.append(average_precision_score(class_sb_best, df_sub['proba_sb_pred']))\n",
    "    ap_ns_list.append(average_precision_score(class_ns_best, df_sub['proba_ns_pred']))\n",
    "    ap_os_list.append(average_precision_score(class_os_best, df_sub['proba_os_pred']))\n",
    "\n",
    "    # get the brier score for the probas and append to the list\n",
    "    brier_sb_list.append(brier_score_loss(class_sb_best, df_sub['proba_sb_pred']))\n",
    "    brier_ns_list.append(brier_score_loss(class_ns_best, df_sub['proba_ns_pred']))\n",
    "    brier_os_list.append(brier_score_loss(class_os_best, df_sub['proba_os_pred']))\n",
    "\n",
    "    # get the mse for the ln's and append to the list\n",
    "    #mse_sb_list.append(mean_squared_error(df_sub['ln_sb_best'], df_sub['ln_sb_pred']))\n",
    "    #mse_ns_list.append(mean_squared_error(df_sub['ln_ns_best'], df_sub['ln_ns_pred']))\n",
    "    #mse_os_list.append(mean_squared_error(df_sub['ln_os_best'], df_sub['ln_os_pred']))\n",
    "\n",
    "    # get the mse for the ln's and append to the list\n",
    "    mse_sb_list.append(mean_squared_error(df_sub['ln_sb_best_oos'], df_sub['ln_sb_pred']))\n",
    "    mse_ns_list.append(mean_squared_error(df_sub['ln_ns_best_oos'], df_sub['ln_ns_pred']))\n",
    "    mse_os_list.append(mean_squared_error(df_sub['ln_os_best_oos'], df_sub['ln_os_pred']))\n",
    "\n",
    "# create a nice table with all the results\n",
    "results_month = pd.DataFrame({\n",
    "    'auroc_sb': auroc_sb_list,\n",
    "    'auroc_ns': auroc_ns_list,\n",
    "    'auroc_os': auroc_os_list,\n",
    "    'ap_sb': ap_sb_list,\n",
    "    'ap_ns': ap_ns_list,\n",
    "    'ap_os': ap_os_list,\n",
    "    'brier_sb': brier_sb_list,\n",
    "    'brier_ns': brier_ns_list,\n",
    "    'brier_os': brier_os_list,\n",
    "    'mse_sb': mse_sb_list,\n",
    "    'mse_ns': mse_ns_list,\n",
    "    'mse_os': mse_os_list,\n",
    "    'moonth_id': month_array\n",
    "}, index=month_array)\n",
    "\n",
    "results_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the 3 aps\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(results_month['moonth_id'], results_month['ap_sb'], label='sb')\n",
    "plt.plot(results_month['moonth_id'], results_month['ap_ns'], label='ns')\n",
    "plt.plot(results_month['moonth_id'], results_month['ap_os'], label='os')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# they are recognizably when compared to the WandB but distintly worse... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So currently I can know if this is right... You need a posterior dict from fimbultuhul which is also evaluation on wieghts and biases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
